<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Notes on probability, stochastic processes, reinforcement learning and games."><meta name=author content=zhengzihao><link href=https://zhengzihao27.github.io/blog/2024/09/27/%E9%87%91%E8%9E%8D%E9%A2%86%E5%9F%9F%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%95/ rel=canonical><link href=../../19/%E9%9D%99%E6%80%81%E9%A1%B5%E9%9D%A2%E7%9B%B8%E5%85%B3/ rel=prev><link href=../../30/%E9%87%91%E8%9E%8D%E9%A2%86%E5%9F%9F%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%95/ rel=next><link rel=icon href=../../../../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.5.34"><title>金融领域强化学习最新进展 - ZhengNotes</title><link rel=stylesheet href=../../../../../assets/stylesheets/main.35f28582.min.css><link rel=stylesheet href=../../../../../assets/stylesheets/palette.06af60db.min.css><style>:root{--md-admonition-icon--note:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M1 7.775V2.75C1 1.784 1.784 1 2.75 1h5.025c.464 0 .91.184 1.238.513l6.25 6.25a1.75 1.75 0 0 1 0 2.474l-5.026 5.026a1.75 1.75 0 0 1-2.474 0l-6.25-6.25A1.75 1.75 0 0 1 1 7.775m1.5 0c0 .066.026.13.073.177l6.25 6.25a.25.25 0 0 0 .354 0l5.025-5.025a.25.25 0 0 0 0-.354l-6.25-6.25a.25.25 0 0 0-.177-.073H2.75a.25.25 0 0 0-.25.25ZM6 5a1 1 0 1 1 0 2 1 1 0 0 1 0-2"/></svg>');--md-admonition-icon--abstract:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M2.5 1.75v11.5c0 .138.112.25.25.25h3.17a.75.75 0 0 1 0 1.5H2.75A1.75 1.75 0 0 1 1 13.25V1.75C1 .784 1.784 0 2.75 0h8.5C12.216 0 13 .784 13 1.75v7.736a.75.75 0 0 1-1.5 0V1.75a.25.25 0 0 0-.25-.25h-8.5a.25.25 0 0 0-.25.25m13.274 9.537zl-4.557 4.45a.75.75 0 0 1-1.055-.008l-1.943-1.95a.75.75 0 0 1 1.062-1.058l1.419 1.425 4.026-3.932a.75.75 0 1 1 1.048 1.074M4.75 4h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1 0-1.5M4 7.75A.75.75 0 0 1 4.75 7h2a.75.75 0 0 1 0 1.5h-2A.75.75 0 0 1 4 7.75"/></svg>');--md-admonition-icon--info:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8m8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13M6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75M8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2"/></svg>');--md-admonition-icon--tip:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M3.499.75a.75.75 0 0 1 1.5 0v.996C5.9 2.903 6.793 3.65 7.662 4.376l.24.202c-.036-.694.055-1.422.426-2.163C9.1.873 10.794-.045 12.622.26 14.408.558 16 1.94 16 4.25c0 1.278-.954 2.575-2.44 2.734l.146.508.065.22c.203.701.412 1.455.476 2.226.142 1.707-.4 3.03-1.487 3.898C11.714 14.671 10.27 15 8.75 15h-6a.75.75 0 0 1 0-1.5h1.376a4.5 4.5 0 0 1-.563-1.191 3.84 3.84 0 0 1-.05-2.063 4.65 4.65 0 0 1-2.025-.293.75.75 0 0 1 .525-1.406c1.357.507 2.376-.006 2.698-.318l.009-.01a.747.747 0 0 1 1.06 0 .75.75 0 0 1-.012 1.074c-.912.92-.992 1.835-.768 2.586.221.74.745 1.337 1.196 1.621H8.75c1.343 0 2.398-.296 3.074-.836.635-.507 1.036-1.31.928-2.602-.05-.603-.216-1.224-.422-1.93l-.064-.221c-.12-.407-.246-.84-.353-1.29a2.4 2.4 0 0 1-.507-.441 3.1 3.1 0 0 1-.633-1.248.75.75 0 0 1 1.455-.364c.046.185.144.436.31.627.146.168.353.305.712.305.738 0 1.25-.615 1.25-1.25 0-1.47-.95-2.315-2.123-2.51-1.172-.196-2.227.387-2.706 1.345-.46.92-.27 1.774.019 3.062l.042.19.01.05c.348.443.666.949.94 1.553a.75.75 0 1 1-1.365.62c-.553-1.217-1.32-1.94-2.3-2.768L6.7 5.527c-.814-.68-1.75-1.462-2.692-2.619a3.7 3.7 0 0 0-1.023.88c-.406.495-.663 1.036-.722 1.508.116.122.306.21.591.239.388.038.797-.06 1.032-.19a.75.75 0 0 1 .728 1.31c-.515.287-1.23.439-1.906.373-.682-.067-1.473-.38-1.879-1.193L.75 5.677V5.5c0-.984.48-1.94 1.077-2.664.46-.559 1.05-1.055 1.673-1.353z"/></svg>');--md-admonition-icon--success:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.75.75 0 0 1 .018-1.042.75.75 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0"/></svg>');--md-admonition-icon--question:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8m8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13M6.92 6.085h.001a.749.749 0 1 1-1.342-.67c.169-.339.436-.701.849-.977C6.845 4.16 7.369 4 8 4a2.76 2.76 0 0 1 1.637.525c.503.377.863.965.863 1.725 0 .448-.115.83-.329 1.15-.205.307-.47.513-.692.662-.109.072-.22.138-.313.195l-.006.004a6 6 0 0 0-.26.16 1 1 0 0 0-.276.245.75.75 0 0 1-1.248-.832c.184-.264.42-.489.692-.661q.154-.1.313-.195l.007-.004c.1-.061.182-.11.258-.161a1 1 0 0 0 .277-.245C8.96 6.514 9 6.427 9 6.25a.61.61 0 0 0-.262-.525A1.27 1.27 0 0 0 8 5.5c-.369 0-.595.09-.74.187a1 1 0 0 0-.34.398M9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0"/></svg>');--md-admonition-icon--warning:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0M9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0"/></svg>');--md-admonition-icon--failure:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M2.344 2.343za8 8 0 0 1 11.314 11.314A8.002 8.002 0 0 1 .234 10.089a8 8 0 0 1 2.11-7.746m1.06 10.253a6.5 6.5 0 1 0 9.108-9.275 6.5 6.5 0 0 0-9.108 9.275M6.03 4.97 8 6.94l1.97-1.97a.749.749 0 0 1 1.275.326.75.75 0 0 1-.215.734L9.06 8l1.97 1.97a.749.749 0 0 1-.326 1.275.75.75 0 0 1-.734-.215L8 9.06l-1.97 1.97a.749.749 0 0 1-1.275-.326.75.75 0 0 1 .215-.734L6.94 8 4.97 6.03a.75.75 0 0 1 .018-1.042.75.75 0 0 1 1.042-.018"/></svg>');--md-admonition-icon--danger:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M9.504.43a1.516 1.516 0 0 1 2.437 1.713L10.415 5.5h2.123c1.57 0 2.346 1.909 1.22 3.004l-7.34 7.142a1.25 1.25 0 0 1-.871.354h-.302a1.25 1.25 0 0 1-1.157-1.723L5.633 10.5H3.462c-1.57 0-2.346-1.909-1.22-3.004zm1.047 1.074L3.286 8.571A.25.25 0 0 0 3.462 9H6.75a.75.75 0 0 1 .694 1.034l-1.713 4.188 6.982-6.793A.25.25 0 0 0 12.538 7H9.25a.75.75 0 0 1-.683-1.06l2.008-4.418.003-.006-.004-.009-.006-.006-.008-.001q-.005 0-.009.004"/></svg>');--md-admonition-icon--bug:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M4.72.22a.75.75 0 0 1 1.06 0l1 .999a3.5 3.5 0 0 1 2.441 0l.999-1a.748.748 0 0 1 1.265.332.75.75 0 0 1-.205.729l-.775.776c.616.63.995 1.493.995 2.444v.327q0 .15-.025.292c.408.14.764.392 1.029.722l1.968-.787a.75.75 0 0 1 .556 1.392L13 7.258V9h2.25a.75.75 0 0 1 0 1.5H13v.5q-.002.615-.141 1.186l2.17.868a.75.75 0 0 1-.557 1.392l-2.184-.873A5 5 0 0 1 8 16a5 5 0 0 1-4.288-2.427l-2.183.873a.75.75 0 0 1-.558-1.392l2.17-.868A5 5 0 0 1 3 11v-.5H.75a.75.75 0 0 1 0-1.5H3V7.258L.971 6.446a.75.75 0 0 1 .558-1.392l1.967.787c.265-.33.62-.583 1.03-.722a1.7 1.7 0 0 1-.026-.292V4.5c0-.951.38-1.814.995-2.444L4.72 1.28a.75.75 0 0 1 0-1.06m.53 6.28a.75.75 0 0 0-.75.75V11a3.5 3.5 0 1 0 7 0V7.25a.75.75 0 0 0-.75-.75ZM6.173 5h3.654A.17.17 0 0 0 10 4.827V4.5a2 2 0 1 0-4 0v.327c0 .096.077.173.173.173"/></svg>');--md-admonition-icon--example:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M5 5.782V2.5h-.25a.75.75 0 0 1 0-1.5h6.5a.75.75 0 0 1 0 1.5H11v3.282l3.666 5.76C15.619 13.04 14.543 15 12.767 15H3.233c-1.776 0-2.852-1.96-1.899-3.458Zm-2.4 6.565a.75.75 0 0 0 .633 1.153h9.534a.75.75 0 0 0 .633-1.153L12.225 10.5h-8.45ZM9.5 2.5h-3V6c0 .143-.04.283-.117.403L4.73 9h6.54L9.617 6.403A.75.75 0 0 1 9.5 6Z"/></svg>');--md-admonition-icon--quote:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M1.75 2.5h10.5a.75.75 0 0 1 0 1.5H1.75a.75.75 0 0 1 0-1.5m4 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5m0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5M2.5 7.75v6a.75.75 0 0 1-1.5 0v-6a.75.75 0 0 1 1.5 0"/></svg>');}</style><style>:root{--md-annotation-icon:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M22 12a10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2a10 10 0 0 1 10 10M6 13h8l-3.5 3.5 1.42 1.42L17.84 12l-5.92-5.92L10.5 7.5 14 11H6z"/></svg>');}</style><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../../../stylesheets/extra.css><script>__md_scope=new URL("../../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><meta property=og:type content=website><meta property=og:title content="金融领域强化学习最新进展 - ZhengNotes"><meta property=og:description content="Notes on probability, stochastic processes, reinforcement learning and games."><meta property=og:image content=https://zhengzihao27.github.io/assets/images/social/blog/posts/论文笔记/强化学习/2.金融领域强化学习最新进展/金融领域强化学习最新进展2023.png><meta property=og:image:type content=image/png><meta property=og:image:width content=1200><meta property=og:image:height content=630><meta content=https://zhengzihao27.github.io/blog/2024/09/27/%E9%87%91%E8%9E%8D%E9%A2%86%E5%9F%9F%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%95/ property=og:url><meta name=twitter:card content=summary_large_image><meta name=twitter:title content="金融领域强化学习最新进展 - ZhengNotes"><meta name=twitter:description content="Notes on probability, stochastic processes, reinforcement learning and games."><meta name=twitter:image content=https://zhengzihao27.github.io/assets/images/social/blog/posts/论文笔记/强化学习/2.金融领域强化学习最新进展/金融领域强化学习最新进展2023.png></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#recent-advances-in-reinforcement-learning-in-finance-1 class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../../../.. title=ZhengNotes class="md-header__button md-logo" aria-label=ZhengNotes data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> ZhengNotes </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> 金融领域强化学习最新进展 </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media=(prefers-color-scheme) data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to system preference" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to system preference" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/zhengzihao27/zhengzihao27.github.io title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> zhengzihao27/zhengzihao27.github.io </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../../../.. class=md-tabs__link> Home </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../../../ class=md-tabs__link> Notes </a> </li> <li class=md-tabs__item> <a href=../../../../../about/ class=md-tabs__link> about me </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation hidden> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../../../.. title=ZhengNotes class="md-nav__button md-logo" aria-label=ZhengNotes data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> ZhengNotes </label> <div class=md-nav__source> <a href=https://github.com/zhengzihao27/zhengzihao27.github.io title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> zhengzihao27/zhengzihao27.github.io </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <div class="md-nav__link md-nav__container"> <a href=../../../../ class="md-nav__link "> <span class=md-ellipsis> Notes </span> </a> <label class="md-nav__link " for=__nav_2 id=__nav_2_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Notes </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2_2> <label class=md-nav__link for=__nav_2_2 id=__nav_2_2_label tabindex> <span class=md-ellipsis> Archive </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2> <span class="md-nav__icon md-icon"></span> Archive </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../archive/2025/ class=md-nav__link> <span class=md-ellipsis> 2025 </span> </a> </li> <li class=md-nav__item> <a href=../../../../archive/2024/ class=md-nav__link> <span class=md-ellipsis> 2024 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2_3> <label class=md-nav__link for=__nav_2_3 id=__nav_2_3_label tabindex> <span class=md-ellipsis> Categories </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3> <span class="md-nav__icon md-icon"></span> Categories </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../category/code/ class=md-nav__link> <span class=md-ellipsis> Code </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/deep-learning/ class=md-nav__link> <span class=md-ellipsis> Deep Learning </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/git/ class=md-nav__link> <span class=md-ellipsis> Git </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/mean-field-game/ class=md-nav__link> <span class=md-ellipsis> Mean Field Game </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/reinforcement-learning/ class=md-nav__link> <span class=md-ellipsis> Reinforcement Learning </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/portfolio-selection/ class=md-nav__link> <span class=md-ellipsis> portfolio selection </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../../../about/ class=md-nav__link> <span class=md-ellipsis> about me </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#2 class=md-nav__link> <span class=md-ellipsis> 2.强化学习基础 </span> </a> <nav class=md-nav aria-label=2.强化学习基础> <ul class=md-nav__list> <li class=md-nav__item> <a href=#21-setupmarkov-decision-processes class=md-nav__link> <span class=md-ellipsis> 2.1 Setup:Markov decision processes </span> </a> <nav class=md-nav aria-label="2.1 Setup:Markov decision processes"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#infinite-time-horizon-and-discounted-reward class=md-nav__link> <span class=md-ellipsis> Infinite time horizon and discounted reward. </span> </a> </li> <li class=md-nav__item> <a href=#finite-time-horizon class=md-nav__link> <span class=md-ellipsis> Finite time horizon. </span> </a> </li> <li class=md-nav__item> <a href=#linear-mdps-and-linear-functional-approximation class=md-nav__link> <span class=md-ellipsis> Linear MDPs and linear functional approximation. </span> </a> </li> <li class=md-nav__item> <a href=#nonlinear-functional-approximation class=md-nav__link> <span class=md-ellipsis> Nonlinear functional approximation. </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#22-from-mdp-to-learning class=md-nav__link> <span class=md-ellipsis> 2.2 From MDP to learning </span> </a> <nav class=md-nav aria-label="2.2 From MDP to learning"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#agent-environment-interface class=md-nav__link> <span class=md-ellipsis> Agent-environment interface. </span> </a> </li> <li class=md-nav__item> <a href=#exploration-versus-exploitation class=md-nav__link> <span class=md-ellipsis> Exploration versus exploitation. </span> </a> </li> <li class=md-nav__item> <a href=#simulator class=md-nav__link> <span class=md-ellipsis> Simulator. </span> </a> </li> <li class=md-nav__item> <a href=#randomized-policy-versus-deterministic-policy class=md-nav__link> <span class=md-ellipsis> Randomized policy versus deterministic policy. </span> </a> </li> <li class=md-nav__item> <a href=#an-example-multi-armed-bandits class=md-nav__link> <span class=md-ellipsis> An example: multi-armed bandits. </span> </a> </li> <li class=md-nav__item> <a href=#221-performance-evaluation class=md-nav__link> <span class=md-ellipsis> 2.2.1 Performance evaluation </span> </a> </li> <li class=md-nav__item> <a href=#222-classification-of-rl-algorithms class=md-nav__link> <span class=md-ellipsis> 2.2.2 Classification of RL algorithms </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#23-value-based-methods class=md-nav__link> <span class=md-ellipsis> 2.3 Value-based methods </span> </a> <nav class=md-nav aria-label="2.3 Value-based methods"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#231-temporal-difference-learning class=md-nav__link> <span class=md-ellipsis> 2.3.1 Temporal-difference learning </span> </a> </li> <li class=md-nav__item> <a href=#232-q-learning-algorithm class=md-nav__link> <span class=md-ellipsis> 2.3.2 Q-learning algorithm </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-content md-content--post" data-md-component=content> <div class="md-sidebar md-sidebar--post" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class="md-sidebar__inner md-post"> <nav class="md-nav md-nav--primary"> <div class=md-post__back> <div class="md-nav__title md-nav__container"> <a href=../../../../ class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> <span class=md-ellipsis> Back to index </span> </a> </div> </div> <div class="md-post__authors md-typeset"> <div class="md-profile md-post__profile"> <span class="md-author md-author--long"> <img src="https://avatars.githubusercontent.com/u/158252341?v=4" alt=zhengzihao> </span> <span class=md-profile__description> <strong> <a href=https://github.com/zhengzihao27>zhengzihao</a> </strong> <br> Creator </span> </div> </div> <ul class="md-post__meta md-nav__list"> <li class="md-nav__item md-nav__item--section"> <div class=md-post__title> <span class=md-ellipsis> Metadata </span> </div> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 19H5V8h14m-3-7v2H8V1H6v2H5c-1.11 0-2 .89-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2h-1V1m-1 11h-5v5h5z"/></svg> <time datetime="2024-09-27 00:00:00" class=md-ellipsis>September 27, 2024</time> </div> </li> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M15 13h1.5v2.82l2.44 1.41-.75 1.3L15 16.69zm4-5H5v11h4.67c-.43-.91-.67-1.93-.67-3a7 7 0 0 1 7-7c1.07 0 2.09.24 3 .67zM5 21a2 2 0 0 1-2-2V5c0-1.11.89-2 2-2h1V1h2v2h8V1h2v2h1a2 2 0 0 1 2 2v6.1c1.24 1.26 2 2.99 2 4.9a7 7 0 0 1-7 7c-1.91 0-3.64-.76-4.9-2zm11-9.85A4.85 4.85 0 0 0 11.15 16c0 2.68 2.17 4.85 4.85 4.85A4.85 4.85 0 0 0 20.85 16c0-2.68-2.17-4.85-4.85-4.85"/></svg> <time datetime="2024-09-27 00:00:00" class=md-ellipsis>September 27, 2024</time> </div> </li> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9 3v15h3V3zm3 2 4 13 3-1-4-13zM5 5v13h3V5zM3 19v2h18v-2z"/></svg> <span class=md-ellipsis> in <a href=../../../../category/reinforcement-learning/ >Reinforcement Learning</a></span> </div> </li> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 20a8 8 0 0 0 8-8 8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8m0-18a10 10 0 0 1 10 10 10 10 0 0 1-10 10C6.47 22 2 17.5 2 12A10 10 0 0 1 12 2m.5 5v5.25l4.5 2.67-.75 1.23L11 13V7z"/></svg> <span class=md-ellipsis> 8 min read </span> </div> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <article class="md-content__inner md-typeset"> <h1 id=recent-advances-in-reinforcement-learning-in-finance-1>Recent advances in reinforcement learning in finance (1)</h1> <p>一篇金融领域强化学习的综述（2021）。经典的随机控制理论在解决金融决策问题上严重依赖模型的假设，而强化学习不同。与经典的方法相比，他不需要太强的模型假设，但是需要大量的金融数据，才能在复杂的金融环境中改进决策。</p> <!-- more --> <p><strong>Autor：</strong> Ben Hambly, Renyuan Xu, Huining Yang</p> <h2 id=2>2.强化学习基础</h2> <h3 id=21-setupmarkov-decision-processes>2.1 Setup:Markov decision processes</h3> <p>马尔科夫决策过程(MDPs, Markov decision processes)是具有马尔科夫性质的随机过程。</p> <p>投资组合优化问题通常按时间是否有限去划分为两类：</p> <ol> <li>Infinite time horizon 的 MDPs。主要研究长期的投资策略，如养老金问题。</li> <li>finite time horizon 的 MDPs。主要研究短期内的交易，如资产购买或清算的最优执行问题， 若目标金额没有被完全执行，则可能会在终止时间受到处罚。</li> </ol> <h4 id=infinite-time-horizon-and-discounted-reward><em>Infinite time horizon and discounted reward.</em></h4> <p>首先考虑<strong>无限时间范围和折扣奖励的</strong>的<strong>离散时间MDP</strong>。</p> <ul> <li>状态空间 <span class=arithmatex>\(\mathcal{S}\)</span>，Markov process取值的空间</li> <li>动作集合 <span class=arithmatex>\(\mathcal{A}\)</span>，通过在集合中采取动作影响演化</li> <li>目的是通过选择一个策略（沿着时间顺序行动）来优化系统的预期回报。</li> </ul> <p>通过定义 value function 来公式化该优化过程</p> <hr> <p>Define value function <span class=arithmatex>\(V^*\)</span> for each <span class=arithmatex>\(s \in \mathcal{S}\)</span> to be</p> <div class=arithmatex>\[\begin{equation} V^{*}(s)=\sup _{\Pi} V^{\Pi}(s):=\sup _{\Pi} \mathbb{E}^{\Pi}\left[\sum_{t=0}^{\infty} \gamma^{t} r\left(s_{t}, a_{t}\right) \mid s_{0}=s\right], \end{equation}\]</div> <p>subject to</p> <div class=arithmatex>\[\begin{equation} s_{t+1} \sim P\left(s_{t}, a_{t}\right), \quad a_{t} \sim \pi_{t}\left(s_{t}\right) . \end{equation}\]</div> <hr> <p>以下做一些记号上的约定：</p> <ul> <li><span class=arithmatex>\(\mathbb{E}^{\Pi}\)</span> 表示在策略 <span class=arithmatex>\({\Pi}\)</span> 下的期望，其中概率测度 <span class=arithmatex>\(\mathcal{P}\)</span> 描述MDP中的动态和奖励。</li> <li>一个空间 <span class=arithmatex>\(\mathcal{X}\)</span> 的概率测度写为 <span class=arithmatex>\(\mathcal{P}(\mathcal{X})\)</span> </li> <li>状态空间 <span class=arithmatex>\((\mathcal{S},d_\mathcal{S})\)</span> 和动作空间 <span class=arithmatex>\((\mathcal{A},d_\mathcal{A})\)</span> 都是完全可分的度量空间，包括 <span class=arithmatex>\(\mathcal{S}\)</span> 和 <span class=arithmatex>\(\mathcal{A}\)</span> 都是有限的情况，这在RL文献中常见。</li> <li><span class=arithmatex>\(\gamma \in (0,1)\)</span> 是一个折现因子</li> <li><span class=arithmatex>\(s_t \in \mathcal{S}\)</span>：<span class=arithmatex>\(t\)</span> 时刻的状态</li> <li><span class=arithmatex>\(a_t \in \mathcal{A}\)</span>：<span class=arithmatex>\(t\)</span> 时刻的动作</li> <li><span class=arithmatex>\(P: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{P}(S)\)</span> 马尔可夫过程下的转移函数</li> <li>记号 <span class=arithmatex>\(s_{t+1} \sim P\left(s_{t}, a_{t}\right)\)</span> 表示 <span class=arithmatex>\(s_{t+1}\)</span> 从分布 <span class=arithmatex>\(P(s_t,a_t)\in \mathcal{P}(\mathcal{S})\)</span> 中进行采样。</li> <li>奖励 <span class=arithmatex>\(r(s,a)\)</span> 是每对<span class=arithmatex>\((s,a)\in \mathcal{S}\times \mathcal{A}\)</span> 的 <span class=arithmatex>\(\mathbb{R}\)</span> 中的一个随机变量</li> <li> <dl> <dt>策略 <span class=arithmatex>\(\Pi=\left\{\pi_{t}\right\}_{t=0}^{\infty}\)</span> 是马尔可夫的，因为它只依赖于当前状态，并且可以确定性的，也可以是随机的。</dt> <dd> <ul> <li>对于确定性的策略，<span class=arithmatex>\(\pi_{t}: \mathcal{S} \rightarrow \mathcal{A}\)</span> 将当前状态 <span class=arithmatex>\(s_t\)</span> 映射到一个确定性的动作</li> </ul> </dd> <dd> <ul> <li>对于随机策略，<span class=arithmatex>\(\pi_{t}: \mathcal{S} \rightarrow \mathcal{P}(\mathcal{A})\)</span> 将当前状态 <span class=arithmatex>\(s_t\)</span> 映射到一个动作空间上的分布。</li> </ul> </dd> <dd> <p>&emsp;&emsp;&emsp; <span class=arithmatex>\(\pi_t(s)\in\mathcal{P}(\mathcal{A})\)</span> 表示给定状态 <span class=arithmatex>\(s\)</span> 在行动空间上的分布</p> </dd> <dd> <p>&emsp;&emsp;&emsp; <span class=arithmatex>\(\pi_t(s,a)\)</span> 表示在状态 <span class=arithmatex>\(s\)</span> 采取动作 <span class=arithmatex>\(a\)</span> 的概率</p> </dd> </dl> </li> </ul> <p>MDPs With infinite-time horizon。在关于 MDPs 的文献中，通常都假设了奖励 <span class=arithmatex>\(r\)</span> 和转移动态 <span class=arithmatex>\(P\)</span> 是时间齐次(time homogeneous)的。</p> <p>此外，在问题的设置上，最大化或者最小化优化目标，本质上都是一致的。</p> <blockquote> <p>动态规划原则(dynamic programming principle, DPP)：最优策略可以通过最大化单步的奖励，然后在新的状态下进行最优化处理，来得到递归方程。</p> </blockquote> <p>我们可以用用动态规划来推导值函数(1)的贝尔曼方程</p> <div class=arithmatex>\[\begin{equation} V^{*}(s)=\sup _{a \in \mathcal{A}}\left\{\mathbb{E}[r(s, a)]+\gamma \mathbb{E}_{s^{\prime} \sim P(s, a)}\left[V^{*}\left(s^{\prime}\right)\right]\right\} . \end{equation}\]</div> <p>引入 Q-function ，我们可以将值函数改写为</p> <div class=arithmatex>\[V^{*}(s)=\sup _{a \in \mathcal{A}} Q^{*}(s, a) ,\]</div> <details class=q-function> <summary>Q-function</summary> <p>Q-function 是强化学习中一个基本量，其定义如下：</p> <div class=arithmatex>\[\begin{equation} Q^{*}(s, a)=\mathbb{E}[r(s, a)]+\gamma \mathbb{E}_{s^{\prime} \sim P(s, a)}\left[V^{*}\left(s^{\prime}\right)\right], \end{equation}\]</div> <p>可以理解为在状态 <span class=arithmatex>\(s\)</span> 下采取动作 <span class=arithmatex>\(a\)</span> 的预期奖励，在此之后遵循最优策略。</p> </details> <p>对于 Q-function，也可以写出相应的贝尔曼方程，如下：</p> <div class=arithmatex>\[\begin{equation} Q^{*}(s, a)=\mathbb{E}[r(s, a)]+\gamma \mathbb{E}_{s^{\prime} \sim P(s, a)} \sup _{a^{\prime} \in \mathcal{A}} Q^{*}\left(s^{\prime}, a^{\prime}\right) . \end{equation}\]</div> <p>这使我们能够从 <span class=arithmatex>\(Q(s,a)\)</span> 中提取最优（平稳）策略 <span class=arithmatex>\(\pi^*(s,a)\)</span>（如果存在的话），具体为</p> <div class=arithmatex>\[\pi^{*}(s, a) \in \arg \max _{a \in \mathcal{A}} Q(s, a) .\]</div> <p>该无限时间问题采取的奖励 reward 设置，对未来的奖励 reward 进行贴现。还有另一种设置是将奖励 reward 进行平均，这也被称为遍历式奖励，但是这种遍历式的奖励通常与金融应用不相关。</p> <h4 id=finite-time-horizon><em>Finite time horizon.</em></h4> <p>有限时间范围 <span class=arithmatex>\(T&lt;\infty\)</span> ，不再贴现未来的价值并且有一个终止奖励。有限时间范围的MDP问题可以表示如下：</p> <div class=arithmatex>\[\begin{equation} V_{t}^{*}(s)=\sup _{\Pi} V_{t}^{\Pi}(s):=\sup _{\Pi} \mathbb{E}^{\Pi}\left[\sum_{u=t}^{T-1} r_{u}\left(s_{u}, a_{u}\right)+r_{T}\left(s_{T}\right) \Bigg| s_{t}=s\right], \forall s \in \mathcal{S}, \end{equation}\]</div> <p>subject to</p> <div class=arithmatex>\[\begin{equation} s_{u+1} \sim P_{u}\left(s_{u}, a_{u}\right), \quad a_{u} \sim \pi_{u}\left(s_{u}\right), \quad t \leq u \leq T-1. \end{equation}\]</div> <p>在无限时间情况的例子中，我们用 <span class=arithmatex>\(s_u\in \mathcal{S}\)</span> 和 <span class=arithmatex>\(a_u\in \mathcal{A}\)</span> 表示 agent 在时间 <span class=arithmatex>\(u\)</span> 的状态和动作。 然而，有限时间与无限时间有一点不同的是，我们允许时间依赖性的转移和奖励函数。</p> <ul> <li><span class=arithmatex>\(P_{u}: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{P}(S)\)</span> 表示转移函数</li> <li><span class=arithmatex>\(r_u(s,a)\)</span> 对每一对<span class=arithmatex>\((s,a)\in \mathcal{S}\times \mathcal{A}\)</span> 是一个实值的随机变量，<span class=arithmatex>\(t\leq u \leq T-1\)</span></li> <li><span class=arithmatex>\(r_T(s)\)</span> 终端奖励，对所有的 <span class=arithmatex>\(s\in\mathcal{S}\)</span> 是一个实值随机变量</li> <li><span class=arithmatex>\(\Pi=\left\{\pi_{u}\right\}_{t=0}^{T}\)</span> 马尔可夫策略可以是确定性的也可以是随机的</li> </ul> <p>值函数(6)对应的贝尔曼方程定义如下：</p> <div class=arithmatex>\[\begin{equation} \begin{split} V_{t}^{*}(s) &amp;=\sup _{a \in \mathcal{A}}\left\{\mathbb{E}\left[r_{t}(s, a)\right]+\mathbb{E}_{s^{\prime} \sim P_{t}(s, a)}\left[V_{t+1}^{*}\left(s^{\prime}\right)\right]\right\}, \\ V_{T}^{*}(s) &amp;=\mathbb{E}\left[r_{T}(s)\right]\ \ \ \ \text{(terminal condition)} \end{split} \end{equation}\]</div> <p>可以将值函数写成</p> <div class=arithmatex>\[V_{t}^{*}(s)=\sup _{a \in \mathcal{A}} Q_{t}^{*}(s, a),\]</div> <p>其中 <span class=arithmatex>\(Q_t^*\)</span> 函数定义如下：</p> <div class=arithmatex>\[\begin{equation} Q_{t}^{*}(s, a)=\mathbb{E}\left[r_{t}(s, a)\right]+\mathbb{E}_{s^{\prime} \sim P_{t}(s, a)}\left[V_{t}^{*}\left(s^{\prime}\right)\right]. \end{equation}\]</div> <p><span class=arithmatex>\(Q\)</span>-function 的贝尔曼方程由下式给出</p> <div class=arithmatex>\[\begin{equation} \begin{split} Q_{t}^{*}(s, a) &amp;=\mathbb{E}\left[r_{t}(s, a)\right]+\mathbb{E}_{s^{\prime} \sim P_{t}(s, a)}\left[\sup _{a^{\prime} \in \mathcal{A}} Q_{t+1}^{*}\left(s^{\prime}, a^{\prime}\right)\right],\\ Q_{T}^{*}(s, a) &amp;=\mathbb{E}\left[r_{T}(s)\right]\ \ \ \ \text{for all } a \in \mathcal{A} \end{split} \end{equation}\]</div> <details class=note> <summary>Note</summary> <p>金融时间序列数据通常是非平稳的，因此(6)和(7)式中的时间变化转移核和价格函数对于金融应用尤为重要。</p> </details> <p>对于一个具有有限状态 <strong>(finite state)</strong> 和动作 <strong>(finite action)</strong> 空间以及有限奖励 <span class=arithmatex>\(r\)</span> <strong>(finite reward)</strong> 的无限时间范围 <strong>(infinite time)</strong> 的马尔可夫决策过程（MDP），只要存在最优策略，该 MDP 总是具有一个平稳的最优策略 (stationary optimal policy)。</p> <blockquote> <p>在一个无限时间跨度的马尔可夫决策过程（MDP）中，如果最优策略存在，那么就可以找到一个静态最优策略，即一个与时间无关的策略，使得在任何时间点应用它都能达到最优结果。即策略不需要随着时间变化。</p> </blockquote> <hr> <p><strong>Theorem 2.1</strong> Theorem 6.2.7 in Puterman (2014). Assume <span class=arithmatex>\(|\mathcal{A}|&lt;\infty\)</span>, <span class=arithmatex>\(|\mathcal{S}|&lt;\infty\)</span>, and <span class=arithmatex>\(|r|&lt;\infty\)</span> with probability one. For any infinite horizon discounted MDP, there always exists a deterministic stationary policy that is optimal.</p> <hr> <p>定理 2.1 意味着总是存在一个固定的策略，可以在每个时间步长执行该策略指定的操作最大化折扣奖励。 agent 不需要随时间改变策略。平均奖励的情况也有一个类似的结果，见 Theorem 8.1.2 in Puterman (2014).</p> <p>这将寻找最优顺序决策策略问题减少到寻找最优平稳策略的问题。因此，记 <span class=arithmatex>\(\pi: \mathcal{S} \rightarrow \mathcal{P}(\mathcal{A})\)</span> (没有时间指数) 为一个平稳策略。</p> <h4 id=linear-mdps-and-linear-functional-approximation><em>Linear MDPs and linear functional approximation.</em></h4> <div class="admonition note"> <p class=admonition-title>Note</p> <p>在线性马尔可夫决策过程（MDP）中，转移核（transition kernels）和奖励函数（reward function）被假设为相对于某些特征映射是线性的（Bradtke &amp; Barto, 1996；Melo &amp; Ribeiro, 2007）。</p> </div> <p>在<strong>无限时间范围的环境</strong>下，若存在 <span class=arithmatex>\(d\)</span> 个在 <span class=arithmatex>\(\mathcal{S}\)</span> 上未知(符号)测度 <span class=arithmatex>\(\mu=\left(\mu^{(1)}, \ldots, \mu^{(d)}\right)\)</span> ，以及一个未知的向量 <span class=arithmatex>\(\theta\in\mathbb{R}^d\)</span>，满足对任意的 <span class=arithmatex>\((s,a)\in \mathcal{S}\times\mathcal{A}\)</span>，有</p> <div class=arithmatex>\[\begin{equation} P(\cdot \mid s, a)=\langle\phi(s, a), \mu(\cdot)\rangle, \quad r(s, a)=\langle\phi(s, a), \theta\rangle . \end{equation}\]</div> <p>则称该MDP为具有特征映射的特征映射 <span class=arithmatex>\(\phi:\mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}^{d}\)</span> 的<strong>线性MDP</strong>。</p> <p>与之相似，在<strong>有限时间环境</strong>下，若对于任意的 <span class=arithmatex>\(0\leq t \leq T\)</span>，存在 <span class=arithmatex>\(d\)</span> 个在 <span class=arithmatex>\(\mathcal{S}\)</span>上的 未知(符号)测度 <span class=arithmatex>\(\mu_{t}=\left(\mu_{t}^{(1)}, \ldots, \mu_{t}^{(d)}\right)\)</span> 以及一个未知的向量 <span class=arithmatex>\(\theta_t \in\mathbb{R}^d\)</span>， 满足对任意的 <span class=arithmatex>\((s,a)\in \mathcal{S}\times\mathcal{A}\)</span>，有</p> <div class=arithmatex>\[\begin{equation} P_{t}(\cdot \mid s, a)=\left\langle\phi(s, a), \mu_{t}(\cdot)\right\rangle, \quad r_{t}(s, a)=\left\langle\phi(s, a), \theta_{t}\right\rangle \end{equation}\]</div> <p>则称该MDP为具有特征映射的特征映射 <span class=arithmatex>\(\phi:\mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}^{d}\)</span> 的<strong>线性MDP</strong>。</p> <p>通常假设这些特征(features)是 agent 已知且有界的，也就是说 <span class=arithmatex>\(\|\phi(s, a)\| \leq 1\)</span> 对所有的 <span class=arithmatex>\((s,a)\in\mathcal{S}\times\mathcal{A}\)</span>。</p> <p>线性MDP框架与具有线性函数逼近的RL的文献密切相关，其中的值函数假设为形式</p> <p>For the infinite horizon case,</p> <div class=arithmatex>\[\begin{equation} Q(s, a)=\langle\psi(s, a), \omega\rangle, \quad V(s)=\langle\xi(s), \eta\rangle \end{equation}\]</div> <p>For the finite horizon case,</p> <div class=arithmatex>\[\begin{equation} Q_{t}(s, a)=\left\langle\psi(s, a), \omega_{t}\right\rangle, \quad V_{t}(s)=\left\langle\xi(s), \eta_{t}\right\rangle, \forall 0 \leq t \leq T \end{equation}\]</div> <ul> <li><span class=arithmatex>\(\psi: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}^{d}\)</span> 和 <span class=arithmatex>\(\xi: \mathcal{S} \rightarrow \mathbb{R}^{d}\)</span> 是已知的特征映射</li> <li><span class=arithmatex>\(\omega, \omega_t,\eta\)</span> 和 <span class=arithmatex>\(\eta_t\)</span> 是未知向量</li> </ul> <p>在温和条件下，线性马尔可夫决策过程(即(11)或(12))与 线性函数逼近形式(即(13)或(14))是等价的（Jin et al., 2020；Yang &amp; Wang, 2019）。</p> <h4 id=nonlinear-functional-approximation><em>Nonlinear functional approximation.</em></h4> <p>与线性函数逼近相比，非线性函数逼近不需要先验的 kernel functions 知识。 非线性函数有可能解决当 agent 对MDP所处的函数空间理解不正确，引起的模型误设问题。 最流行的的非线性函数逼近方法是使用神经网络，从而形成深度强化学习(deep RL)。 得益于<code>Universal approximation theorem</code>定理，这是一种理论上合理的方法， 且神经网络在广泛的应用中表现出良好的性能。 同时，对于某些神经网络架构，基于梯度的算法具有可证明的收敛性保证。</p> <h3 id=22-from-mdp-to-learning>2.2 From MDP to learning</h3> <p>当转移动态 <span class=arithmatex>\(P\)</span> 和奖励函数 <span class=arithmatex>\(r\)</span> 对于无限时间范围的 MDP 问题是未知的，该 MDP 问题就会变为 RL 问题 （即寻找一个最优的平稳策略 <span class=arithmatex>\(\pi\)</span> （如果存在），同时学习未知的 <span class=arithmatex>\(P\)</span> 和 <span class=arithmatex>\(r\)</span> ）。 <span class=arithmatex>\(P\)</span> 和 <span class=arithmatex>\(r\)</span> 的学习可以是显示的，也可以是隐式的，这分别形成了 model-based 和 model-free RL. 类似的思想也适用于有限时间范围。接下来引入一些标准的 RL 术语。</p> <h5 id=agent-environment-interface><em>Agent-environment interface.</em></h5> <dl> <dd>在强化学习（RL）环境中，学习者或决策者称为智能体（agent）。agent 所操作和互动的物理世界，包括 agent 之外的所有事物，称为环境（environment）。 agent 和 environment 在一系列离散时间步骤 <span class=arithmatex>\(t=0,1,2,\dots,\)</span> 中以以下方式进行互动。</dd> </dl> <ul> <li>At the beginning of each time step <span class=arithmatex>\(t\)</span>, agent 接受 environment 状态的某种表示 <span class=arithmatex>\(s_t\in\mathcal{S}\)</span>，并选择一个动作 <span class=arithmatex>\(a_t\in\mathcal{A}\)</span>。</li> <li>At the end of this time step, 作为其动作产生的部分结果，agent 从 environment 接收到一个数值奖励 <span class=arithmatex>\(r_t\)</span>（可能是随机的）和一个新状态 <span class=arithmatex>\(s_{t+1}\)</span>。</li> </ul> <dl> <dd><span class=arithmatex>\((s_t,a_t,r_t,s_{t+1})\)</span> 称为时间 <span class=arithmatex>\(t\)</span> 的一个样本，而 <span class=arithmatex>\(h_{t}:=\left\{\left(s_{u}, a_{u}, r_{u}, s_{u+1}\right)\right\}_{u=0}^{t}\)</span> 被称为截至时间 <span class=arithmatex>\(t\)</span> 的历史或经验。强化学习算法是 agent 与 environment 互动的良定策略的有限序列。</dd> </dl> <h5 id=exploration-versus-exploitation><em>Exploration versus exploitation.</em></h5> <dl> <dd> <dl> <dt>为了最大化时间积累的奖励，agent 需要做出一系列动作。</dt> <dd>利用 (exploitation): agent 基于过去的经验进行学习并选择动作。<br> 探索 (Exploration): agent 尝试做出新的选择。</dd> </dl> </dd> <dd> <p>探索的好处在于能够提供了将当前次优解改进为全局最优解的机会，但这需要耗费时间和计算资源，过度探索可能会削弱算法收敛到最优解的速度。 而不考虑探索的纯利用学习过程，仅基于过去经验短视地选择当前的动作， 虽然易于实现，但却往往导致次优的全局解。 因此，在强化学习算法的设计中，为提升学习和优化表现，需要适当平衡探索与利用。</p> </dd> </dl> <h5 id=simulator><em>Simulator.</em></h5> <dl> <dd>上述描述的 agent 与 environment 的交互过程，agent 是以在线方式与 nvironment 进行交互的。 即，在时间步骤 <span class=arithmatex>\(t+1\)</span> 时的初始状态 <span class=arithmatex>\(s_{t+1}\)</span> 是 agent 从状态 <span class=arithmatex>\(s_t\)</span> 采取动作 <span class=arithmatex>\(a_t\)</span> 后移动到的状态。 这是一个具有挑战性的设置，因为需要有效的探索方案。例如，使用 <span class=arithmatex>\(\epsilon\)</span>-greedy 策略的 <span class=arithmatex>\(Q\)</span>-learning 可能需要经历指数级的时间才能学习到最优策略。 一些研究假设可以访问模拟器，允许算法查询任意的状态-动作对并返回奖励和下一个状态。 这意味着智能体可以在任何时候“重启”系统。也就是说，在时间步骤 <span class=arithmatex>\(t+1\)</span> 时的初始状态 不需要是智能体从状态 <span class=arithmatex>\(s_t\)</span> 采取动作 <span class=arithmatex>\(a_t\)</span> 后移动到的状态。 这个“Simulator”显著减轻了探索的难度， 因为一种简单的探索策略，均匀随机查询所有状态-动作对， 已经能成为寻找最优策略的最有效算法。</dd> </dl> <h5 id=randomized-policy-versus-deterministic-policy><em>Randomized policy versus deterministic policy.</em></h5> <dl> <dd>随机策略 <span class=arithmatex>\(\pi_{t}: \mathcal{S} \rightarrow \mathcal{P}(\mathcal{A})\)</span> 在控制文献中也被称为松弛控制， 在博弈论中也被称为混合策略。 尽管在定理2.1中存在一个确定性的最优策略， 但当 RL agents 对环境不确定时，大多数RL算法都采用随机策略来鼓励探索。</dd> </dl> <h5 id=an-example-multi-armed-bandits><em>An example: multi-armed bandits.</em></h5> <dl> <dd>多臂老虎机模型</dd> </dl> <h4 id=221-performance-evaluation>2.2.1 Performance evaluation</h4> <p>一般是不可能解析求解MDPs的，因此我们将讨论数值算法来确定最优策略。 为了评估不同的算法，我们需要一个衡量它们的性能。这里我们将考虑几种类型的性能度量： 样本复杂度、收敛速度、regret分析和渐近收敛性。</p> <ul> <li>对于有限时间的RL，一个 episode 包含一系列从0开始到时间T的状态、动作、和奖励，用所有 episodes 的样本总数去评估性能。通常称为 episodic RL。 </li> <li>对于无限时间的RL，以步数去评估性能。一步包含一个 (状态，动作，奖励，下一个状态) 的元组。</li> </ul> <blockquote> <p>值得注意的是这种 episodic 准则也可以用来分析无限时间问题。一种流行的做法是在最大时间截断轨迹，将问题转化为近似的有限时间问题。</p> </blockquote> <p>[TODO]</p> <h4 id=222-classification-of-rl-algorithms>2.2.2 Classification of RL algorithms</h4> <p>一个 RL 算法包含以下一个或多个部件：</p> <ul> <li>值函数的表示，提供每个状态或每个状态/动作对有多好的预测。</li> <li>策略 <span class=arithmatex>\(\pi(s)\)</span> 或 <span class=arithmatex>\(\pi(s,a)\)</span> 的直接表示</li> <li>一种环境模型（估计的转移函数和估计的奖励函数），并结合一种规划算法 （使用模型来创建或改进策略的任何计算过程）。</li> </ul> <p>前两个组成部分与所谓的无模型强化学习（model-free RL）相关。 当使用后者组件时，算法被称为基于模型的强化学习（model-based RL）。</p> <p>在马尔可夫决策过程（MDP）设置中， model-based 算法通过估计转移概率和奖励函数来维持一个近似的 MDP 模型，并 从近似的 MDP 中推导出价值函数。然后，从价值函数中导出策略。另一类基于模型的算法则是 对模型进行结构假设，使用一些先验知识，并利用结构信息进行算法设计。</p> <dl> <dt>与 model-based 的方法不同，model-free 算法直接学习价值（或状态-价值）函数或最优策略，</dt> <dt>而无需推断模型。model-free 算法可以进一步分为两类：value-based 和 policy-based 的方法。</dt> <dd> <p>policy-based 方法显式构建策略的表示，并在学习过程中将其保存在内存中。 例子包括策略梯度方法和信任区域策略优化(trust region policy optimization, TRPO)方法。</p> </dd> <dd> <p>value-based 作为一种替代方法，在学习过程中仅存储一个价值函数， 而不显式存储策略。在这种情况下，策略是隐式的，可以直接从价值函数中推导（通过选择具有最佳价值的动作）。</p> </dd> </dl> <h3 id=23-value-based-methods>2.3 Value-based methods</h3> <dl> <dd><strong>Setting:</strong> Infinite time horizon with discounting, finite state and action space <span class=arithmatex>\(|\mathcal{A}|&lt;\infty\)</span> and <span class=arithmatex>\(|\mathcal{S}|&lt;\infty\)</span>, and stationary policies.</dd> </dl> <h4 id=231-temporal-difference-learning>2.3.1 Temporal-difference learning</h4> <p>通过策略 <span class=arithmatex>\(\pi\)</span> 获得一些样本 <span class=arithmatex>\((s,a,r,s')\)</span>，agent 可以在 <span class=arithmatex>\((n+1)th\)</span> 迭代时更新对值函数 <span class=arithmatex>\(V^\pi\)</span> (defined in Equation(1)) 的估计。</p> <div class=arithmatex>\[\begin{equation} V^{\pi,(n+1)}(s) \leftarrow\left(1-\beta_{n}(s, a)\right) \underbrace{V^{\pi,(n)}(s)}_{\text {current estimate }}+\beta_{n}(s, a)[\underbrace{r+\gamma V^{\pi,(n)}\left(s^{\prime}\right)]}_{\text {new estimate }}, \end{equation}\]</div> <ul> <li><span class=arithmatex>\(V^{\pi,(0)}\)</span> 值函数的初始化</li> <li> <p class=annotate><span class=arithmatex>\(\beta_n(s,a)\)</span> (1) 是第 <span class=arithmatex>\(n+1\)</span> 迭代时的学习速率，平衡了当前估计和新估计之间的权重。</p> <ol> <li><span class=arithmatex>\(\beta_n\)</span> 可以是一个常数，也可以依赖于当前状态 <span class=arithmatex>\(s\)</span>，甚至可以依赖于迭代至 <span class=arithmatex>\(n+1\)</span> 次时，已观测到的样本。</li> </ol> </li> </ul> <p>以下是该算法的伪代码，该公式也被称为 TD(0) method，或者 one-step TD method。</p> <p><strong>Algorithm 1</strong> TD(0) Method for estimating <span class=arithmatex>\(V^\pi\)</span> </p> <hr> <p><strong>Input:</strong> total number of iterations <span class=arithmatex>\(N\)</span>; the policy <span class=arithmatex>\(\pi\)</span> used to sample observations; rule to set learning rate <span class=arithmatex>\(\beta_n \in (0,1](0\leq n \leq N-1)\)</span><br> Initialize <span class=arithmatex>\(V^{\pi,(0)}(s)\)</span> for all <span class=arithmatex>\(s\in \mathcal{S}\)</span><br> Initialize <span class=arithmatex>\(s\)</span><br> <strong>for</strong> <span class=arithmatex>\(n=0,\dots,N-1\)</span> <strong>do</strong><br> &emsp;Sample action <span class=arithmatex>\(a\)</span> according to <span class=arithmatex>\(\pi(s)\)</span><br> &emsp;Observe <span class=arithmatex>\(r\)</span> and <span class=arithmatex>\(s'\)</span> after taking action <span class=arithmatex>\(a\)</span><br> &emsp;Update <span class=arithmatex>\(V^{\pi,(n+1)}\)</span> according to (2.21) with <span class=arithmatex>\((s,a,r,s')\)</span><br> &emsp;<span class=arithmatex>\(s\gets s'\)</span><br> <strong>end for</strong></p> <hr> <details class=note> <summary>Note</summary> <p>TD(<span class=arithmatex>\(\lambda\)</span>) method 结合了两种常见的强化学习方法：<strong>时序差分学习 (TD learning)</strong>(权重 <span class=arithmatex>\(\lambda\)</span> ) 和<strong>Monte Carlo method</strong> (权重 <span class=arithmatex>\(1-\lambda\)</span>)。 </p> <ol> <li>TD learning：逐步更新的强化学习方法。它通过对当前状态和未来状态的估计差（时间差分）进行更新。TD 学习适合在没有完整轨迹信息的情况下进行学习。 </li> <li>蒙特卡洛方法：基于整个轨迹的更新方法，必须等到一个完整的轨迹结束之后，才能根据实际的回报来更新估计。它可以使用更多历史信息进行更新。 </li> </ol> <p>TD(<span class=arithmatex>\(\lambda\)</span>) 使用权重参数 <span class=arithmatex>\(\lambda\)</span> 来平衡两种方法:</p> <ul> <li>当 <span class=arithmatex>\(\lambda = 1\)</span> 时，TD<span class=arithmatex>\((\lambda)\)</span> 变为蒙特卡洛方法，因为它完全依赖于完整的轨迹。 </li> <li>当 <span class=arithmatex>\(\lambda = 0\)</span> 时，TD(𝜆) 变为普通的 TD learning，因为它只使用单步更新。 </li> <li><span class=arithmatex>\(\lambda\)</span> 介于 0 和 1 之间时，它是两种方法的组合，既利用了时间差分学习的快速更新，又利用了蒙特卡洛方法对长期回报的考虑。 </li> </ul> <p>简单来说，TD<span class=arithmatex>\((\lambda)\)</span>是一个权衡短期更新和长期更新的混合方法。</p> </details> <p>TD 更新方程可以重写为：</p> <div class=arithmatex>\[\begin{equation} V^{\pi,(n+1)}(s) \leftarrow V^{\pi,(n)}(s)+\beta_{n}(s, a) \left[\underbrace{\overbrace{r+\gamma V^{\pi,(n)}\left(s^{\prime}\right)}^{\mathrm{TD} \text { target }}-V^{\pi,(n)}(s)}_{\text {TD error } \delta_{n}}\right] . \end{equation}\]</div> <ul> <li><span class=arithmatex>\(\delta_n\)</span>: TD error ，测量在当前状态 <span class=arithmatex>\(s\)</span> 的估计值好更好的估计(TD target) <span class=arithmatex>\(r+\gamma V^{\pi,(n)}(s')\)</span> 之间的差值。</li> <li>TD error 和TD target 是分析近似值函数收敛性的两个重要组成部分</li> </ul> <h4 id=232-q-learning-algorithm>2.3.2 Q-learning algorithm</h4> <p>[TODO]</p> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../../19/%E9%9D%99%E6%80%81%E9%A1%B5%E9%9D%A2%E7%9B%B8%E5%85%B3/ class="md-footer__link md-footer__link--prev" aria-label="Previous: 静态页面相关"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> 静态页面相关 </div> </div> </a> <a href=../../30/%E9%87%91%E8%9E%8D%E9%A2%86%E5%9F%9F%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%95/ class="md-footer__link md-footer__link--next" aria-label="Next: 金融领域强化学习最新进展"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> 金融领域强化学习最新进展 </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2016 - 2024 Zhengzihao </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <script id=__config type=application/json>{"base": "../../../../..", "features": ["navigation.indexes", "navigation.instant", "navigation.instant.progress", "navigation.tabs", "navigation.expand", "navigation.sections", "navigation.footer", "navigation.top", "navigation.tracking", "navigation.prune", "content.tabs.link", "content.code.copy", "content.code.select", "content.tooltips", "search.highlight", "search.share", "search.suggest"], "search": "../../../../../assets/javascripts/workers/search.07f07601.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../../../../../assets/javascripts/bundle.56dfad97.min.js></script> <script src=../../../../../javascripts/mathjax.js></script> <script src=https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js></script> <script src=https://unpkg.com/mermaid@10.6.1/dist/mermaid.min.js></script> </body> </html>