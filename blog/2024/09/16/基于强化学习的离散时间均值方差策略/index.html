<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Notes on probability, stochastic processes, reinforcement learning and games."><meta name=author content=zhengzihao><link href=https://zhengzihao27.github.io/blog/2024/09/16/%E5%9F%BA%E4%BA%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%A6%BB%E6%95%A3%E6%97%B6%E9%97%B4%E5%9D%87%E5%80%BC%E6%96%B9%E5%B7%AE%E7%AD%96%E7%95%A5/ rel=canonical><link href=../../13/%E7%89%B9%E5%BE%81%E6%8E%92%E5%BA%8F%E5%9B%A0%E5%AD%90%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/ rel=prev><link href=../../19/%E9%9D%99%E6%80%81%E9%A1%B5%E9%9D%A2%E7%9B%B8%E5%85%B3/ rel=next><link rel=icon href=../../../../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.5.34"><title>基于强化学习的离散时间均值方差策略 - ZhengNotes</title><link rel=stylesheet href=../../../../../assets/stylesheets/main.35f28582.min.css><link rel=stylesheet href=../../../../../assets/stylesheets/palette.06af60db.min.css><style>:root{--md-admonition-icon--note:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M1 7.775V2.75C1 1.784 1.784 1 2.75 1h5.025c.464 0 .91.184 1.238.513l6.25 6.25a1.75 1.75 0 0 1 0 2.474l-5.026 5.026a1.75 1.75 0 0 1-2.474 0l-6.25-6.25A1.75 1.75 0 0 1 1 7.775m1.5 0c0 .066.026.13.073.177l6.25 6.25a.25.25 0 0 0 .354 0l5.025-5.025a.25.25 0 0 0 0-.354l-6.25-6.25a.25.25 0 0 0-.177-.073H2.75a.25.25 0 0 0-.25.25ZM6 5a1 1 0 1 1 0 2 1 1 0 0 1 0-2"/></svg>');--md-admonition-icon--abstract:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M2.5 1.75v11.5c0 .138.112.25.25.25h3.17a.75.75 0 0 1 0 1.5H2.75A1.75 1.75 0 0 1 1 13.25V1.75C1 .784 1.784 0 2.75 0h8.5C12.216 0 13 .784 13 1.75v7.736a.75.75 0 0 1-1.5 0V1.75a.25.25 0 0 0-.25-.25h-8.5a.25.25 0 0 0-.25.25m13.274 9.537zl-4.557 4.45a.75.75 0 0 1-1.055-.008l-1.943-1.95a.75.75 0 0 1 1.062-1.058l1.419 1.425 4.026-3.932a.75.75 0 1 1 1.048 1.074M4.75 4h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1 0-1.5M4 7.75A.75.75 0 0 1 4.75 7h2a.75.75 0 0 1 0 1.5h-2A.75.75 0 0 1 4 7.75"/></svg>');--md-admonition-icon--info:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8m8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13M6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75M8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2"/></svg>');--md-admonition-icon--tip:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M3.499.75a.75.75 0 0 1 1.5 0v.996C5.9 2.903 6.793 3.65 7.662 4.376l.24.202c-.036-.694.055-1.422.426-2.163C9.1.873 10.794-.045 12.622.26 14.408.558 16 1.94 16 4.25c0 1.278-.954 2.575-2.44 2.734l.146.508.065.22c.203.701.412 1.455.476 2.226.142 1.707-.4 3.03-1.487 3.898C11.714 14.671 10.27 15 8.75 15h-6a.75.75 0 0 1 0-1.5h1.376a4.5 4.5 0 0 1-.563-1.191 3.84 3.84 0 0 1-.05-2.063 4.65 4.65 0 0 1-2.025-.293.75.75 0 0 1 .525-1.406c1.357.507 2.376-.006 2.698-.318l.009-.01a.747.747 0 0 1 1.06 0 .75.75 0 0 1-.012 1.074c-.912.92-.992 1.835-.768 2.586.221.74.745 1.337 1.196 1.621H8.75c1.343 0 2.398-.296 3.074-.836.635-.507 1.036-1.31.928-2.602-.05-.603-.216-1.224-.422-1.93l-.064-.221c-.12-.407-.246-.84-.353-1.29a2.4 2.4 0 0 1-.507-.441 3.1 3.1 0 0 1-.633-1.248.75.75 0 0 1 1.455-.364c.046.185.144.436.31.627.146.168.353.305.712.305.738 0 1.25-.615 1.25-1.25 0-1.47-.95-2.315-2.123-2.51-1.172-.196-2.227.387-2.706 1.345-.46.92-.27 1.774.019 3.062l.042.19.01.05c.348.443.666.949.94 1.553a.75.75 0 1 1-1.365.62c-.553-1.217-1.32-1.94-2.3-2.768L6.7 5.527c-.814-.68-1.75-1.462-2.692-2.619a3.7 3.7 0 0 0-1.023.88c-.406.495-.663 1.036-.722 1.508.116.122.306.21.591.239.388.038.797-.06 1.032-.19a.75.75 0 0 1 .728 1.31c-.515.287-1.23.439-1.906.373-.682-.067-1.473-.38-1.879-1.193L.75 5.677V5.5c0-.984.48-1.94 1.077-2.664.46-.559 1.05-1.055 1.673-1.353z"/></svg>');--md-admonition-icon--success:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.75.75 0 0 1 .018-1.042.75.75 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0"/></svg>');--md-admonition-icon--question:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8m8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13M6.92 6.085h.001a.749.749 0 1 1-1.342-.67c.169-.339.436-.701.849-.977C6.845 4.16 7.369 4 8 4a2.76 2.76 0 0 1 1.637.525c.503.377.863.965.863 1.725 0 .448-.115.83-.329 1.15-.205.307-.47.513-.692.662-.109.072-.22.138-.313.195l-.006.004a6 6 0 0 0-.26.16 1 1 0 0 0-.276.245.75.75 0 0 1-1.248-.832c.184-.264.42-.489.692-.661q.154-.1.313-.195l.007-.004c.1-.061.182-.11.258-.161a1 1 0 0 0 .277-.245C8.96 6.514 9 6.427 9 6.25a.61.61 0 0 0-.262-.525A1.27 1.27 0 0 0 8 5.5c-.369 0-.595.09-.74.187a1 1 0 0 0-.34.398M9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0"/></svg>');--md-admonition-icon--warning:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0M9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0"/></svg>');--md-admonition-icon--failure:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M2.344 2.343za8 8 0 0 1 11.314 11.314A8.002 8.002 0 0 1 .234 10.089a8 8 0 0 1 2.11-7.746m1.06 10.253a6.5 6.5 0 1 0 9.108-9.275 6.5 6.5 0 0 0-9.108 9.275M6.03 4.97 8 6.94l1.97-1.97a.749.749 0 0 1 1.275.326.75.75 0 0 1-.215.734L9.06 8l1.97 1.97a.749.749 0 0 1-.326 1.275.75.75 0 0 1-.734-.215L8 9.06l-1.97 1.97a.749.749 0 0 1-1.275-.326.75.75 0 0 1 .215-.734L6.94 8 4.97 6.03a.75.75 0 0 1 .018-1.042.75.75 0 0 1 1.042-.018"/></svg>');--md-admonition-icon--danger:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M9.504.43a1.516 1.516 0 0 1 2.437 1.713L10.415 5.5h2.123c1.57 0 2.346 1.909 1.22 3.004l-7.34 7.142a1.25 1.25 0 0 1-.871.354h-.302a1.25 1.25 0 0 1-1.157-1.723L5.633 10.5H3.462c-1.57 0-2.346-1.909-1.22-3.004zm1.047 1.074L3.286 8.571A.25.25 0 0 0 3.462 9H6.75a.75.75 0 0 1 .694 1.034l-1.713 4.188 6.982-6.793A.25.25 0 0 0 12.538 7H9.25a.75.75 0 0 1-.683-1.06l2.008-4.418.003-.006-.004-.009-.006-.006-.008-.001q-.005 0-.009.004"/></svg>');--md-admonition-icon--bug:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M4.72.22a.75.75 0 0 1 1.06 0l1 .999a3.5 3.5 0 0 1 2.441 0l.999-1a.748.748 0 0 1 1.265.332.75.75 0 0 1-.205.729l-.775.776c.616.63.995 1.493.995 2.444v.327q0 .15-.025.292c.408.14.764.392 1.029.722l1.968-.787a.75.75 0 0 1 .556 1.392L13 7.258V9h2.25a.75.75 0 0 1 0 1.5H13v.5q-.002.615-.141 1.186l2.17.868a.75.75 0 0 1-.557 1.392l-2.184-.873A5 5 0 0 1 8 16a5 5 0 0 1-4.288-2.427l-2.183.873a.75.75 0 0 1-.558-1.392l2.17-.868A5 5 0 0 1 3 11v-.5H.75a.75.75 0 0 1 0-1.5H3V7.258L.971 6.446a.75.75 0 0 1 .558-1.392l1.967.787c.265-.33.62-.583 1.03-.722a1.7 1.7 0 0 1-.026-.292V4.5c0-.951.38-1.814.995-2.444L4.72 1.28a.75.75 0 0 1 0-1.06m.53 6.28a.75.75 0 0 0-.75.75V11a3.5 3.5 0 1 0 7 0V7.25a.75.75 0 0 0-.75-.75ZM6.173 5h3.654A.17.17 0 0 0 10 4.827V4.5a2 2 0 1 0-4 0v.327c0 .096.077.173.173.173"/></svg>');--md-admonition-icon--example:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M5 5.782V2.5h-.25a.75.75 0 0 1 0-1.5h6.5a.75.75 0 0 1 0 1.5H11v3.282l3.666 5.76C15.619 13.04 14.543 15 12.767 15H3.233c-1.776 0-2.852-1.96-1.899-3.458Zm-2.4 6.565a.75.75 0 0 0 .633 1.153h9.534a.75.75 0 0 0 .633-1.153L12.225 10.5h-8.45ZM9.5 2.5h-3V6c0 .143-.04.283-.117.403L4.73 9h6.54L9.617 6.403A.75.75 0 0 1 9.5 6Z"/></svg>');--md-admonition-icon--quote:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M1.75 2.5h10.5a.75.75 0 0 1 0 1.5H1.75a.75.75 0 0 1 0-1.5m4 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5m0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5M2.5 7.75v6a.75.75 0 0 1-1.5 0v-6a.75.75 0 0 1 1.5 0"/></svg>');}</style><style>:root{--md-annotation-icon:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M22 12a10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2a10 10 0 0 1 10 10M6 13h8l-3.5 3.5 1.42 1.42L17.84 12l-5.92-5.92L10.5 7.5 14 11H6z"/></svg>');}</style><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../../../stylesheets/extra.css><script>__md_scope=new URL("../../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><meta property=og:type content=website><meta property=og:title content="基于强化学习的离散时间均值方差策略 - ZhengNotes"><meta property=og:description content="Notes on probability, stochastic processes, reinforcement learning and games."><meta property=og:image content=https://zhengzihao27.github.io/assets/images/social/blog/posts/1.论文笔记/强化学习/1.基于强化学习的离散时间均值方差策略/基于强化学习的离散时间均值方差策略.png><meta property=og:image:type content=image/png><meta property=og:image:width content=1200><meta property=og:image:height content=630><meta content=https://zhengzihao27.github.io/blog/2024/09/16/%E5%9F%BA%E4%BA%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%A6%BB%E6%95%A3%E6%97%B6%E9%97%B4%E5%9D%87%E5%80%BC%E6%96%B9%E5%B7%AE%E7%AD%96%E7%95%A5/ property=og:url><meta name=twitter:card content=summary_large_image><meta name=twitter:title content="基于强化学习的离散时间均值方差策略 - ZhengNotes"><meta name=twitter:description content="Notes on probability, stochastic processes, reinforcement learning and games."><meta name=twitter:image content=https://zhengzihao27.github.io/assets/images/social/blog/posts/1.论文笔记/强化学习/1.基于强化学习的离散时间均值方差策略/基于强化学习的离散时间均值方差策略.png></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#discrete-time-mean-variance-strategy-based-on-reinforcement-learning class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../../../.. title=ZhengNotes class="md-header__button md-logo" aria-label=ZhengNotes data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> ZhengNotes </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> 基于强化学习的离散时间均值方差策略 </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media=(prefers-color-scheme) data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to system preference" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to system preference" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/zhengzihao27/zhengzihao27.github.io title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> zhengzihao27/zhengzihao27.github.io </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../../../.. class=md-tabs__link> Home </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../../../ class=md-tabs__link> Notes </a> </li> <li class=md-tabs__item> <a href=../../../../../about/ class=md-tabs__link> about me </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation hidden> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../../../.. title=ZhengNotes class="md-nav__button md-logo" aria-label=ZhengNotes data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> ZhengNotes </label> <div class=md-nav__source> <a href=https://github.com/zhengzihao27/zhengzihao27.github.io title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> zhengzihao27/zhengzihao27.github.io </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <div class="md-nav__link md-nav__container"> <a href=../../../../ class="md-nav__link "> <span class=md-ellipsis> Notes </span> </a> <label class="md-nav__link " for=__nav_2 id=__nav_2_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Notes </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2_2> <label class=md-nav__link for=__nav_2_2 id=__nav_2_2_label tabindex> <span class=md-ellipsis> Archive </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2> <span class="md-nav__icon md-icon"></span> Archive </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../archive/2025/ class=md-nav__link> <span class=md-ellipsis> 2025 </span> </a> </li> <li class=md-nav__item> <a href=../../../../archive/2024/ class=md-nav__link> <span class=md-ellipsis> 2024 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2_3> <label class=md-nav__link for=__nav_2_3 id=__nav_2_3_label tabindex> <span class=md-ellipsis> Categories </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3> <span class="md-nav__icon md-icon"></span> Categories </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../category/code/ class=md-nav__link> <span class=md-ellipsis> Code </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/deep-learning/ class=md-nav__link> <span class=md-ellipsis> Deep Learning </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/git/ class=md-nav__link> <span class=md-ellipsis> Git </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/mean-field-game/ class=md-nav__link> <span class=md-ellipsis> Mean Field Game </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/portfolio-selection/ class=md-nav__link> <span class=md-ellipsis> Portfolio Selection </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/reinforcement-learning/ class=md-nav__link> <span class=md-ellipsis> Reinforcement Learning </span> </a> </li> <li class=md-nav__item> <a href=../../../../category/stochastic-analysis/ class=md-nav__link> <span class=md-ellipsis> Stochastic Analysis </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../../../about/ class=md-nav__link> <span class=md-ellipsis> about me </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#2 class=md-nav__link> <span class=md-ellipsis> 2 离散时间探索性的均值方差问题 </span> </a> <nav class=md-nav aria-label="2 离散时间探索性的均值方差问题"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#21 class=md-nav__link> <span class=md-ellipsis> 2.1 经典离散时间均值方差问题 </span> </a> </li> <li class=md-nav__item> <a href=#22 class=md-nav__link> <span class=md-ellipsis> 2.2 离散时间探索性的均值方差问题 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#3 class=md-nav__link> <span class=md-ellipsis> 3. 离散时间算法 </span> </a> <nav class=md-nav aria-label="3. 离散时间算法"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#31 class=md-nav__link> <span class=md-ellipsis> 3.1 策略提升定理和策略收敛 </span> </a> </li> <li class=md-nav__item> <a href=#32 class=md-nav__link> <span class=md-ellipsis> 3.2 算法设计 </span> </a> <nav class=md-nav aria-label="3.2 算法设计"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#policy-evaluation class=md-nav__link> <span class=md-ellipsis> Policy Evaluation </span> </a> </li> <li class=md-nav__item> <a href=#policy-improvement class=md-nav__link> <span class=md-ellipsis> Policy Improvement </span> </a> </li> <li class=md-nav__item> <a href=#self-correcting-scheme class=md-nav__link> <span class=md-ellipsis> Self-correcting Scheme </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#4 class=md-nav__link> <span class=md-ellipsis> 4.伪代码 </span> </a> </li> <li class=md-nav__item> <a href=#5result class=md-nav__link> <span class=md-ellipsis> 5.Result </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-content md-content--post" data-md-component=content> <div class="md-sidebar md-sidebar--post" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class="md-sidebar__inner md-post"> <nav class="md-nav md-nav--primary"> <div class=md-post__back> <div class="md-nav__title md-nav__container"> <a href=../../../../ class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> <span class=md-ellipsis> Back to index </span> </a> </div> </div> <div class="md-post__authors md-typeset"> <div class="md-profile md-post__profile"> <span class="md-author md-author--long"> <img src="https://avatars.githubusercontent.com/u/158252341?v=4" alt=zhengzihao> </span> <span class=md-profile__description> <strong> <a href=https://github.com/zhengzihao27>zhengzihao</a> </strong> <br> Creator </span> </div> </div> <ul class="md-post__meta md-nav__list"> <li class="md-nav__item md-nav__item--section"> <div class=md-post__title> <span class=md-ellipsis> Metadata </span> </div> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 19H5V8h14m-3-7v2H8V1H6v2H5c-1.11 0-2 .89-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2h-1V1m-1 11h-5v5h5z"/></svg> <time datetime="2024-09-16 00:00:00" class=md-ellipsis>September 16, 2024</time> </div> </li> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M15 13h1.5v2.82l2.44 1.41-.75 1.3L15 16.69zm4-5H5v11h4.67c-.43-.91-.67-1.93-.67-3a7 7 0 0 1 7-7c1.07 0 2.09.24 3 .67zM5 21a2 2 0 0 1-2-2V5c0-1.11.89-2 2-2h1V1h2v2h8V1h2v2h1a2 2 0 0 1 2 2v6.1c1.24 1.26 2 2.99 2 4.9a7 7 0 0 1-7 7c-1.91 0-3.64-.76-4.9-2zm11-9.85A4.85 4.85 0 0 0 11.15 16c0 2.68 2.17 4.85 4.85 4.85A4.85 4.85 0 0 0 20.85 16c0-2.68-2.17-4.85-4.85-4.85"/></svg> <time datetime="2024-10-16 00:00:00" class=md-ellipsis>October 16, 2024</time> </div> </li> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9 3v15h3V3zm3 2 4 13 3-1-4-13zM5 5v13h3V5zM3 19v2h18v-2z"/></svg> <span class=md-ellipsis> in <a href=../../../../category/reinforcement-learning/ >Reinforcement Learning</a></span> </div> </li> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 20a8 8 0 0 0 8-8 8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8m0-18a10 10 0 0 1 10 10 10 10 0 0 1-10 10C6.47 22 2 17.5 2 12A10 10 0 0 1 12 2m.5 5v5.25l4.5 2.67-.75 1.23L11 13V7z"/></svg> <span class=md-ellipsis> 11 min read </span> </div> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <article class="md-content__inner md-typeset"> <h1 id=discrete-time-mean-variance-strategy-based-on-reinforcement-learning>Discrete-Time Mean-Variance Strategy Based on Reinforcement Learning</h1> <p>根据贝尔曼最优原则求解带熵正则的最优值函数，并给出相应的最优策略分布。后给定最初容许控制，证明策略评估后的策略迭代过程能够最终收敛到最优策略分布，并参数化值函数和策略分布，设计RL算法，根据历史金融数据进行学习并估计参数。</p> <!-- more --> <p><strong>Autor：</strong> Xiangyu Cui, Xun Li, Yun Shi, Si Zhao</p> <h2 id=2>2 离散时间探索性的均值方差问题</h2> <ul> <li>2.1 经典离散时间均值方差问题</li> <li>2.2 离散时间探索性的均值方差问题</li> </ul> <h3 id=21>2.1 经典离散时间均值方差问题</h3> <p>首先考虑由两个资产组成的市场：</p> <ul> <li>无风险资产，回报率 <span class=arithmatex>\(r_f\)</span></li> <li>风险资产，t到t+1的超额回报率 <span class=arithmatex>\(r_t\)</span>，服从均值为 <span class=arithmatex>\(a\)</span>，方差为 <span class=arithmatex>\(\sigma^2\)</span> 的正态分布。</li> </ul> <p><strong>假设：</strong> <span class=arithmatex>\(r_t,t=0,1,\dots,T-1\)</span> 是统计上独立的</p> <ul> <li>投资者进入市场的初始财富为 <span class=arithmatex>\(x_0\)</span></li> <li>投资者希望在策略 <span class=arithmatex>\(\mathbf{u} = \{u_0, u_1,\dots,u_{T-1} \}\)</span> 下的最终财富期望值为 <span class=arithmatex>\(x_T = b\)</span></li> </ul> <p>简单说，投资者将会面临以下的优化问题：</p> <div class=arithmatex>\[\begin{align*} \min\ &amp;\operatorname{Var}(x_T^u), \\[2mm] \text{s.t. }&amp;\mathbb{E}[x_T^u] = b,\\[2mm] &amp;x_{t+1}= r_f x_t + r_t u_t,\quad\quad t = 0,1,\dots,T-1 \end{align*}\]</div> <p>引入 Lagrange multiplier <span class=arithmatex>\(w\)</span>，将问题转换为无约束条件问题。</p> <div class=arithmatex>\[\begin{equation} \min_u\ \mathbb{E}(x_T^u)^2-b^2-2w(\mathbb{E}[x_T^u]-b) = \min_u\ \mathbb{E}(x_T^u - w)^2 - (w-b)^2. \end{equation}\]</div> <p>该问题存在解析解 <span class=arithmatex>\(\mathbf{u}^*=\{u_0^*,u_1^*,\dots,u_{T-1}^*\}\)</span> 依赖于 <span class=arithmatex>\(w\)</span>. 详细的推导可见 <em>(2000)Optimal dynamic portfolio selection: Multiperiod mean-variance formulation</em></p> <h3 id=22>2.2 离散时间探索性的均值方差问题</h3> <p>在强化学习框架中，控制过程 <span class=arithmatex>\(\mathbf{u} = \{u_t,0\le t &lt;T\}\)</span> 是随机化的，代表探索和学习，从而形成了以测度值或分布控制的过程，其密度函数表示为 <span class=arithmatex>\(\boldsymbol{\pi} = \{\pi_t,0\le t &lt; T\}\)</span>. 因此，财富的动态表示为：</p> <div class=arithmatex>\[\begin{equation} x_{t+1}^\pi = r_f x_t^\pi + r_t u_t^\pi. \end{equation}\]</div> <p><strong>假定：</strong></p> <ul> <li>超额收益 <span class=arithmatex>\(r_t\)</span> 服从一个均值为 <span class=arithmatex>\(a\)</span> 和方差 <span class=arithmatex>\(\sigma^2\)</span> 的分布。</li> <li><span class=arithmatex>\(u_t^\pi\)</span> 是一个随机控制过程，概率密度为 <span class=arithmatex>\(\pi_t\)</span>。</li> <li><span class=arithmatex>\(r_t\)</span> 与 <span class=arithmatex>\(u_t^\pi\)</span> 是独立的。</li> </ul> <p>那么在周期 <span class=arithmatex>\(t\)</span> 处，<span class=arithmatex>\(r_t u_t^\pi\)</span> 的条件一阶矩和二阶矩可以表示为：</p> <div class=arithmatex>\[\begin{align*} &amp;\mathbb{E}_t[r_tu_t^\pi] = \mathbb{E}_t[r_t]\mathbb{E}_t[u_t^\pi] = a\int_{\mathbb{R}}u\pi_t(u)du,\\ &amp;\mathbb{E}_t[(r_tu_t^\pi)^2]=\mathbb{E}_t[(r_t)^2]\mathbb{E}_t[(u_t^\pi)^2]=(a^2+\sigma^2)\int_{\mathbb{R}}u^2\pi_t(u)du. \end{align*}\]</div> <p>随机分布控制过程 <span class=arithmatex>\(\boldsymbol{\pi} =\{\pi_t,0 \le t &lt; T\}\)</span> 用总体的累积熵去刻画的强化学习的探索过程。</p> <div class=arithmatex>\[\begin{equation*} \mathcal{H}(\boldsymbol{\pi}) := -\sum_{t=0}^{T-1}\int_{\mathbb{R}}\pi_t(u)\ln\pi_t(u)du. \end{equation*}\]</div> <p>在离散时间市场环境下，探索性MV问题的目标函数变为：</p> <div class=arithmatex>\[\begin{equation} V^\pi = \min_\pi \mathbb{E}\left[(x_T^\pi - w)^2 + \lambda \sum_{t=0}^{T-1}\int_{\mathbb{R}}\pi_t(u)\ln\pi_t(u)du\right] - (w-b)^2. \end{equation}\]</div> <details class=原先的目标函数> <summary>原先的目标函数</summary> <div class=arithmatex>\[\begin{equation*} \min_u\ \mathbb{E}(x_T^u - w)^2 - (w-b)^2. \end{equation*}\]</div> </details> <ul> <li><span class=arithmatex>\(\lambda :\)</span> temperature parameter measures the trade-off between exploitation and exploration in this MV problem.</li> </ul> <p>问题(3)中理论的最优反馈控制和相应的最优值函数如下：</p> <p>定义在策略 <span class=arithmatex>\(\boldsymbol{\pi}\)</span> 下的值函数 <span class=arithmatex>\(J(t,x;w)\)</span>:</p> <div class=arithmatex>\[\begin{equation*} J(t,x;w) = \mathbb{E}\left[(x_T^\pi - w)^2 + \lambda \sum_{s=t}^{T-1}\int_{\mathbb{R}}\pi_s(u)\ln\pi_s(u)du \mid x_t^\pi = x \right] - (w-b)^2 \end{equation*}\]</div> <p><span class=arithmatex>\(J^*(t,x;w)\)</span> 称为问题(3)的最优值函数：</p> <div class=arithmatex>\[\begin{align*} &amp;J^*(t,x;w) = \min_{\pi_{t}, \ldots, \pi_{T-1}}\mathbb{E}\left[(x_T^\pi - w)^2 + \lambda \sum_{s=t}^{T-1}\int_{\mathbb{R}}\pi_s(u)\ln\pi_s(u)du \mid x_t^\pi = x \right] - (w-b)^2,\\ &amp;J^*(T,x;w) = (x-w)^2 - (w-b)^2. \end{align*}\]</div> <hr> <p><strong>Theorem 1.</strong> At period t, the optimal value function is given by</p> <div class=arithmatex>\[\begin{equation} \begin{split} &amp;J^*(t,x;w) \\ =&amp;\left(\frac{\sigma^2 r_f^2}{a^2+\sigma^2}\right)^{T-t}(x-\rho_t w)^2 + \frac{\lambda}{2}(T-t)\ln\left(\frac{a^2 + \sigma^2}{\pi \lambda}\right) \\ &amp;+ \frac{\lambda}{2}\sum_{i=t+1}^{T}(T-i)\ln\left(\frac{\sigma^2 r_f^2}{a^2 + \sigma^2}\right) - (w-b)^2. \end{split} \end{equation}\]</div> <p>where <span class=arithmatex>\(\rho_t = (r_f^{-1})^{T-t}\)</span>. Moreover, the optimal feedback control is Gaussian, with its density function given by</p> <div class=arithmatex>\[\begin{equation} \pi^*(u;t,x,w) = \mathcal{N}\left(u \,\Bigg| \, - \frac{ar_f(x-\rho_t w)}{a^2 + \sigma^2}, \frac{\lambda}{2(a^2 + \sigma^2)}\left( \frac{a^2 + \sigma^2}{\sigma^2 r_f^2} \right)^{T-t-1}\right) . \end{equation}\]</div> <hr> <p>[NOTE:超额收益 <span class=arithmatex>\(r_t\)</span> 服从一个均值为 <span class=arithmatex>\(a\)</span> 和方差 <span class=arithmatex>\(\sigma^2\)</span> 的分布。]</p> <details class=定理1有两点需要注意> <summary>定理1有两点需要注意</summary> <ol> <li>方差项衡量最优的高斯策略在 <span class=arithmatex>\(t\)</span> 时刻的的探索程度，意味着探索会随着时间衰减。agent 初始会最大程度的进行探索，然后探索程度会随时间逐渐衰减。 随着时间接近到期日，利用会主导探索并且会变得越来越重要，这是因为有一个将对投资者的行动进行评估的 deadline T。</li> <li>最优高斯策略的均值与探索权重 <span class=arithmatex>\(\lambda\)</span> 无关，方差与状态 <span class=arithmatex>\(x\)</span> 无关。完美的将<strong>利用</strong>和<strong>探索</strong>进行了分离，因为前者由均值捕捉，后者由方差捕捉。</li> </ol> </details> <h2 id=3>3. 离散时间算法</h2> <p>上述内容证明并给出了离散时间探索性均值-方差问题的最优解。接下来设计对应的RL算法来 直接学习求解和输出投资组合分配策略。接下来要做的内容有两部分：</p> <ul> <li>1.策略提升定理和策略收敛</li> <li>2.一个自校正方案来学习真正的拉格朗日乘子 <span class=arithmatex>\(w\)</span> </li> </ul> <details class=注意> <summary>注意</summary> <p>该RL算法跳过了模型参数的估计，如超额回报 <span class=arithmatex>\(r_t\)</span> 的均值和方差，这些参数难以准确估计。</p> </details> <h3 id=31>3.1 策略提升定理和策略收敛</h3> <details class=为什么需要策略提升定理和策略收敛> <summary>为什么需要策略提升定理和策略收敛</summary> <p>策略方案可以向正确的方向更新当前策略，以改进 value function 。 <strong>策略提升定理</strong>会保证迭代后的 value function 是非增的（在最小化的问题的情况下）。 <strong>策略收敛</strong>保证会最终收敛到 optimal value function 。</p> </details> <p>以下给出的定理说明了，在固定 <span class=arithmatex>\(w\)</span> 和给定服从以下分布的最初的容许控制的情况下，最终策略会收敛到最优策略</p> <hr> <p><strong>Theorem 2.</strong> Suppose <span class=arithmatex>\(w\in \mathbb{R}\)</span> is fixed and <span class=arithmatex>\(\boldsymbol{\pi}^0\)</span> is an arbitrarily given admissible feedback control policy, subjecting to</p> <div class=arithmatex>\[ \pi_t^0(u;x,w) = \mathcal{N}\left( u \mid K(x-\rho_t w), \lambda BC^{T-t-1} \right) \]</div> <p>Then we can calculate <span class=arithmatex>\(J^{\pi^0}(t,x;w)\)</span></p> <div class=arithmatex>\[\begin{align*} \text{where} &amp;\\ &amp; \rho_t = r_f^{-(T-t)} \\ &amp; A = r_f^2 + (a^2 + \sigma^2)K^2 + 2r_f a K \\ &amp; f(t) = \frac{\lambda B (a^2 + \sigma^2)[1-(CA)^{T-t}]}{1-CA} - \frac{\lambda}{2}\ln (2\pi \lambda B)(T-t) - \frac{\lambda}{2}(T-t)-\frac{\lambda}{2}\ln C\sum_{i=0}^{T-t-1}i-(w-b)^2 \end{align*}\]</div> <p><span class=arithmatex>\(f(t)\)</span> is a smooth function that only depends on <span class=arithmatex>\(t\)</span></p> <div class=arithmatex>\[\begin{equation*} \begin{split} &amp;J^{\pi^0}(t,x;w) \\ = &amp; A^{T-t}(x-\rho_t w)^2 + \frac{\lambda B (a^2 + \sigma^2)[1-(CA)^{T-t}]}{1-CA} - \frac{\lambda}{2}\ln (2\pi \lambda B)(T-t)\\ &amp; - \frac{\lambda}{2}(T-t)-\frac{\lambda}{2}\ln C\sum_{i=0}^{T-t-1}i-(w-b)^2 \\ = &amp; A^{T-t}(x-\rho_t w)^2 + f(t) \end{split} \end{equation*}\]</div> <p>Using the condition <span class=arithmatex>\(\pi_{t}^{k+1}(u ; x, w)=\arg \min _{\pi_{t}^{k}(u)} J^{\pi^{k}}(t, x ; w)\)</span> to update the feedback policy and making this iteration for k times, we can get <span class=arithmatex>\(\pi_{t}^{k}(u ; x, w)\)</span> and the corresponding value function <span class=arithmatex>\(J^{\pi^{k}}(t, x ; w):\)</span></p> <div class=arithmatex>\[\begin{equation} \pi_{t}^{k}(u ; x, w)=\mathcal{N}\left(u \left\lvert\,-\frac{a r_{f}\left(x-\rho_{t} w\right)}{a^{2}+\sigma^{2}}\right., \frac{\lambda}{2\left(a^{2}+\sigma^{2}\right) A^{T-t-k}}\left(\frac{a^{2}+\sigma^{2}}{\sigma^{2} r_{f}^{2}}\right)^{k-1} \right), \end{equation}\]</div> <div class=arithmatex>\[\begin{equation} \begin{split} &amp; J^{\pi^k}(t,x;w) \\ = &amp; A^{T-t-k}\left(\frac{\sigma^{2} r_{f}^{2}}{a^{2}+\sigma^{2}}\right)^{k}\left(x-\rho_{t} w\right)^{2}+\frac{\lambda}{2} k \ln \left(\frac{a^{2}+\sigma^{2}}{\pi \lambda}\right) \\ &amp; + \frac{\lambda}{2} \sum_{i=0}^{k-1} i \ln \left(\frac{\sigma^{2} r_{f}^{2}}{a^{2}+\sigma^{2}}\right) + \frac{\lambda \ln A}{2} k(T-t-k)+f(t+k). \end{split} \end{equation}\]</div> <hr> <details class=note> <summary>Note</summary> <p>上述定理描述了以下迭代过程是合理的。即：</p> <p>&emsp; <strong>策略评估：</strong>给定当前策略 <span class=arithmatex>\(\pi^k\)</span> 计算其对应的值函数 <span class=arithmatex>\(J^{\pi^k}\)</span></p> <p>&emsp; <strong>策略提升：</strong>选择让 <span class=arithmatex>\(J^{\pi^k}\)</span> 最小化的策略分布：</p> <div class=arithmatex>\[\pi_{t}^{k+1}(u ; x, w)=\arg \min _{\pi_{t}^{k}(u)} J^{\pi^{k}}(t, x ; w) \]</div> <p>该定理证明了，该迭代过程，能够使更新后的策略能够收敛到理论最优策略。</p> </details> <h3 id=32>3.2 算法设计</h3> <p>设计RL算法去求解离散时间探索性投资组合选择问题。算法包含三个步骤：策略评估，策略提升和学习 Lagrange multiplier <span class=arithmatex>\(w\)</span> 的自校正方案。</p> <h4 id=policy-evaluation>Policy Evaluation</h4> <p>通过 Bellman equation 可以得到</p> <div class=arithmatex>\[ J^{\pi}(t, x ; w)=\mathbb{E}\left[J^{\pi}\left(t+1, x_{t+1} ; w\right)+\lambda \int_{\mathbb{R}} \pi_{t}(u) \ln \pi_{t}(u) d u \Bigg| x_{t}=x\right] \]</div> <p>将方程进行重排，得到</p> <div class=arithmatex>\[\mathbb{E}\left[J^{\pi}\left(t+1, x_{t+1} ; w\right)-J^{\pi}\left(t, x_{t} ; w\right)+\lambda \int_{\mathbb{R}} \pi_{t}(u) \ln \pi_{t}(u) d u \Bigg| x_{t}=x\right]=0 \]</div> <p>定义 Bellman's error</p> <div class=arithmatex>\[\delta_{t}=\hat{J}_{t}^{\pi}+\lambda \int_{\mathbb{R}} \pi_{t}(u) \ln \pi_{t}(u) d u \]</div> <p>其中 <span class=arithmatex>\(\hat{J}_{t}^{\pi}=J^{\pi}\left(t+1, x_{t+1} ; w\right)-J^{\pi}\left(t, x_{t} ; w\right)\)</span>.</p> <p>策略评估的目标是要最小化贝尔曼误差 <span class=arithmatex>\(\delta_t\)</span>. <img alt=❓ class=twemoji src=https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.0.3/assets/svg/2753.svg title=:question:></p> <ul> <li><span class=arithmatex>\(J^\theta\)</span> 参数化的 value function，<span class=arithmatex>\(\theta\)</span> 需要学习的参数化向量</li> <li><span class=arithmatex>\(\pi^\phi\)</span> 参数化的 policy，<span class=arithmatex>\(\phi\)</span> 需要学习的参数化向量</li> </ul> <p>然后最小化 </p> <div class=arithmatex>\[\begin{align*} C(\theta, \phi) &amp; =\frac{1}{2} \mathbb{E}\left[\sum_{i=0}^{T-1}\left|\delta_{i}\right|^{2}\right] \\ &amp; =\frac{1}{2} \mathbb{E}\left[\sum_{i=0}^{T-1}\left|J^{\theta}\left(i+1, x_{i+1} ; w\right)-J^{\theta}\left(i, x_{i} ; w\right)+\lambda \int_{\mathbb{R}} \pi_{i}^{\phi}(u) \ln \pi_{i}^{\phi}(u) d u\right|^{2}\right] \\ &amp; =\frac{1}{2} \mathbb{E}\left[\sum_{i=0}^{T-1}\left|\hat{J}_{i}^{\theta}+\lambda \int_{\mathbb{R}} \pi_{i}^{\phi}(u) \ln \pi_{i}^{\phi}(u) d u\right|^{2}\right] \end{align*}\]</div> <p>其中 <span class=arithmatex>\(\pi^{\phi}=\left\{\pi_{t}^{\phi}, t=0,1, \ldots, T-1\right\}\)</span>. </p> <p>用以下方式收集样本 <span class=arithmatex>\(D=\{(t,x_t),t=0,1,\dots,T \}\)</span></p> <p><span class=arithmatex>\(t=0\)</span>，初始样本 <span class=arithmatex>\((0,x_0)\)</span></p> <p><span class=arithmatex>\(t=0,1,\dots,T-1\)</span>，采样 <span class=arithmatex>\(\pi_t^\phi\)</span> 获得在风险资产的分配 <span class=arithmatex>\(u_t\)</span>， 并且观察 <span class=arithmatex>\(t+1\)</span> 时刻的财富值 <span class=arithmatex>\(x_{t+1}\)</span></p> <p>通过以下方式去近似 <span class=arithmatex>\(C(\theta,\phi)\)</span></p> <div class=arithmatex>\[\begin{align*} C(\theta, \phi) &amp; =\frac{1}{2} \sum_{\left(t, x_{t}\right) \in D}\left[J^{\theta}\left(t+1, x_{t+1} ; w\right)-J^{\theta}\left(t, x_{t} ; w\right)+\lambda \int_{\mathbb{R}} \pi_{t}^{\phi}(u) \ln \pi_{t}^{\phi}(u) d u\right]^{2} \\ &amp; =\frac{1}{2} \sum_{\left(t, x_{t}\right) \in D}\left[\hat{J}^{\theta}\left(t, x_{t} ; w\right)+\lambda \int_{\mathbb{R}} \pi_{t}^{\phi}(u) \ln \pi_{t}^{\phi}(u) d u\right]^{2} \end{align*}\]</div> <p>根据先前求出的最优值函数，我们可以考虑将值函数参数化为：</p> <div class=arithmatex>\[J^{\theta}(t, x ; w)=\theta_{1}^{T-t}\left(x-\rho_{t} w\right)^{2}+\theta_{2} t^{2}+\theta_{3} t+\theta_{4}, \]</div> <details class="recall optimal value function"> <summary>Recall</summary> <div class=arithmatex>\[\begin{align*} &amp;J^*(t,x;w) \\ =&amp;\left(\frac{\sigma^2 r_f^2}{a^2+\sigma^2}\right)^{T-t}(x-\rho_t w)^2 + \frac{\lambda}{2}(T-t)\ln\left(\frac{a^2 + \sigma^2}{\pi \lambda}\right) \\ &amp;+ \frac{\lambda}{2}\sum_{i=t+1}^{T}(T-i)\ln\left(\frac{\sigma^2 r_f^2}{a^2 + \sigma^2}\right) - (w-b)^2. \end{align*}\]</div> </details> <p>其中 <span class=arithmatex>\(\rho_t = r_f^{-(T-t)}\)</span> 和 <span class=arithmatex>\(r_f\)</span> 都是已知的。</p> <h4 id=policy-improvement>Policy Improvement</h4> <p>从策略提升更新方案中，策略 <span class=arithmatex>\(\pi_t^{\phi}(u)\)</span> 的均值和方差分别是，<span class=arithmatex>\(-\frac{a r_{f}\left(x-\rho_{t} w\right)}{a^{2}+\sigma^{2}}\)</span> 和 <span class=arithmatex>\(\frac{\lambda}{2\left(a^{2}+\sigma^{2}\right) \theta_{1}^{T-t-1}}\)</span> 由此，可以得到熵</p> <div class=arithmatex>\[\begin{align*} H\left(\pi_{t}^{\phi}\right) &amp; =-\int_{\mathbb{R}} \pi_{t}^{\phi}(u) \ln \pi_{t}^{\phi}(u) d u \\ &amp; =\frac{1}{2} \ln \left(\frac{\pi e \lambda}{a^{2}+\sigma^{2}}\right)-\frac{\ln \theta_{1}}{2}(T-t-1) \end{align*}\]</div> <p>将其进行参数化，表示为 <span class=arithmatex>\(H\left(\pi_{t}^{\phi}\right)=\phi_{1}+\phi_{2}(T-t-1)\)</span> ，可以导出：</p> <div class=arithmatex>\[ \left\{\begin{array}{l} \phi_{1}=\frac{1}{2} \ln \left(\frac{\pi e \lambda}{a^{2}+\sigma^{2}}\right) \\ \phi_{2}=-\frac{\ln \theta_{1}}{2} \\ \theta_{1}=\frac{\sigma^{2} r_{f}^{2}}{a^{2}+\sigma^{2}} \end{array}\right. \]</div> <p>用 <span class=arithmatex>\(\phi_1\)</span> 和 <span class=arithmatex>\(\phi_2\)</span> 重新表示策略</p> <div class=arithmatex>\[\begin{align*} &amp; \pi_{t}^{\phi}(u ; x, w) \\ = &amp; \mathcal{N}\left(u \left\lvert\,-\frac{a r_{f}\left(x-\rho_{t} w\right)}{a^{2}+\sigma^{2}}\right., \frac{\lambda}{2\left(a^{2}+\sigma^{2}\right) \theta_{1}^{T-t-1}}\right) \\ = &amp; \mathcal{N}\left(u \left\lvert\,-\sqrt{\frac{r_{f}^{2}-e^{-2 \phi_{2}}}{\lambda \pi}} e^{\frac{2 \phi_{1}-1}{2}}\left(x-\rho_{t} w\right)\right., \frac{1}{2 \pi} e^{2 \phi_{2}(T-t-1)+2 \phi_{1}-1}\right) \end{align*}\]</div> <p>利用上述得到的参数化形式的 <span class=arithmatex>\(H\left(\pi_{t}^{\phi}\right)=\phi_{1}+\phi_{2}(T-t-1)\)</span>，重写 <span class=arithmatex>\(C(\theta,\phi)\)</span> 得到</p> <div class=arithmatex>\[C(\theta, \phi)=\frac{1}{2} \sum_{\left(t, x_{t}\right) \in D}\left[\hat{J}^{\theta}\left(t, x_{t} ; w\right)-\lambda\left(\phi_{1}+\phi_{2}(T-t-1)\right)\right]^{2} \]</div> <p>该论文采用随机梯度下降（SGD）的方式去更新参数 <span class=arithmatex>\((\phi_1,\phi_2)'\)</span> 和 <span class=arithmatex>\((\theta_2,\theta_3)'\)</span></p> <div class=arithmatex>\[\begin{align*} \textbf{Note that:} &amp; \\ &amp; \hat{J}^{\theta}\left(t, x_{t} ; w\right)=J^{\theta}\left(t+1, x_{t+1} ; w\right)-J^{\theta}\left(t, x_{t} ; w\right) \\ &amp; J^{\theta}\left(t, x_{t} ; w\right)\ 中的参数\ \theta_{1}=e^{-2 \phi_{2}} &amp; \end{align*}\]</div> <div class=arithmatex>\[\begin{equation} \frac{\partial C}{\partial \theta_{2}}=\sum_{\left(t, x_{t}\right) \in D}\left[\hat{J}^{\theta}\left(t, x_{t} ; w\right)-\lambda\left(\phi_{1}+\phi_{2}(T-t-1)\right)\right]\left((t+1)^{2}-t^{2}\right), \end{equation}\]</div> <div class=arithmatex>\[\begin{equation} \frac{\partial C}{\partial \theta_{3}}=\sum_{\left(t, x_{t}\right) \in D}\left[\hat{J}^{\theta}\left(t, x_{t} ; w\right)-\lambda\left(\phi_{1}+\phi_{2}(T-t-1)\right)\right], \end{equation}\]</div> <div class=arithmatex>\[\begin{equation} \frac{\partial C}{\partial \phi_{1}}=-\lambda \sum_{\left(t, x_{t}\right) \in D}\left[\hat{J}^{\theta}\left(t, x_{t} ; w\right)-\lambda\left(\phi_{1}+\phi_{2}(T-t-1)\right)\right], \end{equation}\]</div> <div class=arithmatex>\[\begin{equation} \begin{split} \frac{\partial C}{\partial \phi_{2}}= &amp;\sum_{\left(t, x_{t}\right) \in D}\left[\hat{J}^{\theta}\left(t, x_{t} ; w\right)-\lambda\left(\phi_{1}+\phi_{2}(T-t-1)\right)\right] \times \\ &amp; \left[2(T-t) e^{-2 \phi_{2}(T-t)}\left(x_{t}-\rho_{t} w\right)^{2}\right. \\ &amp; \left.-2(T-t-1) e^{-2 \phi_{2}(T-t-1)}\left(x_{t+1}-\rho_{t+1} w\right)^{2}-\lambda(T-t-1)\right]. \end{split} \end{equation}\]</div> <p><span class=arithmatex>\(\theta_1\)</span> 根据 <span class=arithmatex>\(\theta_1 = e^{-2\phi_2}\)</span>，<span class=arithmatex>\(\theta_4\)</span> 则根据终值条件进行更新</p> <div class=arithmatex>\[\begin{align*} J^{\theta}(T, x ; w) &amp;=\left(x- w\right)^{2}+\theta_{2} T^{2}+\theta_{3} T+\theta_{4} \\ &amp; =(x-w)^{2}-(w-b)^{2} \end{align*}\]</div> <p>换句话说 <span class=arithmatex>\((\theta_1,\theta_4)\)</span> 的更新方案如下：</p> <ul> <li><span class=arithmatex>\(\theta_1 = e^{-2\phi_2}\)</span> </li> <li><span class=arithmatex>\(\theta_{4}=-\theta_{2} T^{2}-\theta_{3} T-(w-b)^{2}\)</span> </li> </ul> <h4 id=self-correcting-scheme>Self-correcting Scheme</h4> <div class=arithmatex>\[w_{n+1}=w_{n}-\alpha\left(x_{T}-b\right)\]</div> <p>其中 <span class=arithmatex>\(\alpha\)</span> 是拉格朗日乘子自校正的学习率。</p> <p>在实现过程中，将 <span class=arithmatex>\(x_T\)</span> 替换为样本均值 <span class=arithmatex>\(\frac{1}{N} \sum_{j} x_{T}^{j}\)</span></p> <ul> <li><span class=arithmatex>\(N\)</span> 是样本大小</li> <li><span class=arithmatex>\(\left\{x_{T}^{j}\right\}\)</span> 是 <span class=arithmatex>\(w\)</span> 更新后获得的最近 <span class=arithmatex>\(N\)</span> 个的终值财富值</li> </ul> <h2 id=4>4.伪代码</h2> <p><figure markdown=span> <img alt="Image title" src=../../../../1.%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/1.%E5%9F%BA%E4%BA%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%A6%BB%E6%95%A3%E6%97%B6%E9%97%B4%E5%9D%87%E5%80%BC%E6%96%B9%E5%B7%AE%E7%AD%96%E7%95%A5/images/Algorithum_1.png width=auto> </figure></p> <details class=tip> <summary>Tip</summary> <p>强化学习的目标是通过找到一个最优策略，最大化累积奖励。通常有两类方法：</p> <ol> <li><strong>基于值函数的方法</strong>（价值迭代、策略迭代）：通过学习状态值函数 <span class=arithmatex>\(V^\pi(s)\)</span>，反过来从值函数推导出最优策略。</li> <li><strong>基于策略的方法</strong>（策略梯度）：直接优化策略，不依赖显示的值函数。</li> </ol> <dl> <dt><strong>1.策略迭代的进本框架</strong></dt> <dd> <p><strong>策略评估：</strong>给定当前策略 <span class=arithmatex>\(\pi^k\)</span>，计算其对应的值函数 <span class=arithmatex>\(V^{\pi^k}(s)\)</span></p> </dd> <dd> <p><strong>策略改进：</strong>根据值函数，更新策略 <span class=arithmatex>\(\pi^{k+1}\)</span>，例如选择让 <span class=arithmatex>\(V^{\pi^k}(s)\)</span> 最大化的策略分布：</p> </dd> </dl> <div class=arithmatex>\[\pi^{k+1}(s) = \arg\max_{\pi^k} V^{\pi^k}(s) \]</div> <dl> <dd><strong>迭代：</strong>重复以上步骤，直到策略收敛</dd> </dl> </details> <h2 id=5result>5.Result</h2> <p>论文复现代码过程，查阅<a href=../../../10/16/discrete-time-exploratory-mv-algorithm/ >此处</a></p> <p>论文复现实验结果如下：</p> <table> <thead> <tr> <th>Target wealth b</th> <th>a = 30%, sigma = 20%</th> </tr> </thead> <tbody> <tr> <td>b = 1.1</td> <td>mean = 0.1078, std = 0.1661, SR = 0.6488</td> </tr> <tr> <td>b = 1.07</td> <td>mean = 0.0717, std = 0.1144, SR = 0.6263</td> </tr> <tr> <td>b = 1.05</td> <td>mean = 0.0522, std = 0.0764, SR = 0.6833</td> </tr> </tbody> </table> <table> <thead> <tr> <th>Target wealth b</th> <th>a = 30%, sigma = 30%</th> </tr> </thead> <tbody> <tr> <td>b = 1.1</td> <td>mean = 0.1033, std = 0.1201, SR = 0.8604</td> </tr> <tr> <td>b = 1.07</td> <td>mean = 0.0710, std = 0.0921, SR = 0.7707</td> </tr> <tr> <td>b = 1.05</td> <td>mean = 0.0502, std = 0.0665, SR = 0.7546</td> </tr> </tbody> </table> <figure> <img alt="b=1.05, a=0.3, sigma=0.2" src=../../../../1.%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/1.%E5%9F%BA%E4%BA%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%A6%BB%E6%95%A3%E6%97%B6%E9%97%B4%E5%9D%87%E5%80%BC%E6%96%B9%E5%B7%AE%E7%AD%96%E7%95%A5/images/Learning%20Curves_1.05_0.3_0.2.png width=auto> </figure> <figure> <img alt="b=1.07, a=0.3, sigma=0.2" src=../../../../1.%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/1.%E5%9F%BA%E4%BA%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%A6%BB%E6%95%A3%E6%97%B6%E9%97%B4%E5%9D%87%E5%80%BC%E6%96%B9%E5%B7%AE%E7%AD%96%E7%95%A5/images/Learning%20Curves_1.07_0.3_0.2.png width=auto> </figure> <figure> <img alt="b=1.1, a=0.3, sigma=0.2" src=../../../../1.%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/1.%E5%9F%BA%E4%BA%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%A6%BB%E6%95%A3%E6%97%B6%E9%97%B4%E5%9D%87%E5%80%BC%E6%96%B9%E5%B7%AE%E7%AD%96%E7%95%A5/images/Learning%20Curves_1.1_0.3_0.2.png width=auto> </figure> <figure> <img alt="b=1.05, a=0.3, sigma=0.3" src=../../../../1.%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/1.%E5%9F%BA%E4%BA%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%A6%BB%E6%95%A3%E6%97%B6%E9%97%B4%E5%9D%87%E5%80%BC%E6%96%B9%E5%B7%AE%E7%AD%96%E7%95%A5/images/Learning%20Curves_1.05_0.3_0.3.png width=auto> </figure> <figure> <img alt="b=1.07, a=0.3, sigma=0.3" src=../../../../1.%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/1.%E5%9F%BA%E4%BA%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%A6%BB%E6%95%A3%E6%97%B6%E9%97%B4%E5%9D%87%E5%80%BC%E6%96%B9%E5%B7%AE%E7%AD%96%E7%95%A5/images/Learning%20Curves_1.07_0.3_0.3.png width=auto> </figure> <figure> <img alt="b=1.1, a=0.3, sigma=0.3" src=../../../../1.%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/1.%E5%9F%BA%E4%BA%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%A6%BB%E6%95%A3%E6%97%B6%E9%97%B4%E5%9D%87%E5%80%BC%E6%96%B9%E5%B7%AE%E7%AD%96%E7%95%A5/images/Learning%20Curves_1.1_0.3_0.3.png width=auto> </figure> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../../13/%E7%89%B9%E5%BE%81%E6%8E%92%E5%BA%8F%E5%9B%A0%E5%AD%90%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/ class="md-footer__link md-footer__link--prev" aria-label="Previous: 特征排序因子模型中的深度学习"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> 特征排序因子模型中的深度学习 </div> </div> </a> <a href=../../19/%E9%9D%99%E6%80%81%E9%A1%B5%E9%9D%A2%E7%9B%B8%E5%85%B3/ class="md-footer__link md-footer__link--next" aria-label="Next: 静态页面相关"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> 静态页面相关 </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2016 - 2024 Zhengzihao </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <script id=__config type=application/json>{"base": "../../../../..", "features": ["navigation.indexes", "navigation.instant", "navigation.instant.progress", "navigation.tabs", "navigation.expand", "navigation.sections", "navigation.footer", "navigation.top", "navigation.tracking", "navigation.prune", "content.tabs.link", "content.code.copy", "content.code.select", "content.tooltips", "search.highlight", "search.share", "search.suggest"], "search": "../../../../../assets/javascripts/workers/search.07f07601.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../../../../../assets/javascripts/bundle.56dfad97.min.js></script> <script src=../../../../../javascripts/mathjax.js></script> <script src=https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js></script> <script src=https://unpkg.com/mermaid@10.6.1/dist/mermaid.min.js></script> </body> </html>